{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ah44kDXQYaxt"
   },
   "source": [
    "# Taxi Demand Prediction - New York City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPU9O3YyYaxv"
   },
   "source": [
    "## Business/Real World Problem\n",
    "For a given location in New York City, our goal is to <b>predict the number of pickups in that given location</b>. Some location require more taxis at a particular time than other locations owing to the presence schools, hospitals, offices etc. The prediction result can be transferred to the taxi drivers via Smartphone app, and they can subsequently move to the locations where predicted pickups are high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HfQNOUiYaxw"
   },
   "source": [
    "## Objectives & Constraints\n",
    "<b>Objectives:</b> Our objective is to predict the number of pickups as accurately as possible for each region in a 10min interval. We will break up the whole New York City into regions. Now, the 10min interval is chosen because in NYC one can commute 1 mile in approximately 10 minutes given the traffic is normal at that particular time.<br><br>\n",
    "<b>Constraints:</b>\n",
    "* <b>Latency:</b> Given a location and current time of a taxi driver, as a taxi driver, he/she excepts to get the predicted pickups in his/her region and the adjoining regions in few seconds. Hence, there is a medium latency requirement.<br><br>\n",
    "\n",
    "* <b>Interpretability:</b> As long as taxi driver gets good prediction result, he/she is not be much interested in the interpretability of the result. He/she is not much interested in why he/she is getting this result. Hence, there is a no interpretability required.<br><br>\n",
    "\n",
    "* <b>Relative Errors:</b> Mean Absolute Percentage Error will be the relative error we will consider. Let say the predicted pickups for a particular location are 100, but actual pickups are 102, the percentage error will be 2% and Absolute error is 2. The taxi driver will be more interested in the percentage error than the absolute error. Let say in some region the predicted pickups are 250, and if taxi driver knows that the relative error is 10% then he/she will consider the predicted result to be in the range of 225 to 275, which is considerable.<br><br>\n",
    "\n",
    "<b>Our goal is to reduce the percentage error is low as possible.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1SB2XGcYaxx"
   },
   "source": [
    "# 1. Data Information\n",
    "<b>Source of Data:</b> Data can be downloaded from here:<br>\n",
    "http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml.<br> \n",
    "Here, we have used Jan- 2015 and Jan- 2016 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3W7CBNVmYayF"
   },
   "source": [
    "## Data Collection\n",
    "\n",
    "We have collected all yellow taxi trips data of Jan-2015 and Jan-2016(Will be using only Jan 2015 data)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>File-Name</th>\n",
    "        <th>File-Size</th>\n",
    "        <th>Number of Records</th>\n",
    "        <th>Number of Features</th>\n",
    "        </tr>\n",
    "    <tr>\n",
    "        <td> yellow_tripdata_2015-01</td>\n",
    "        <td> 1.84GB</td>\n",
    "        <td> 12748986</td>\n",
    "        <td> 19 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> yellow_tripdata_2016-01</td>\n",
    "        <td> 1.59GB</td>\n",
    "        <td> 10906858</td>\n",
    "        <td> 19 </td>\n",
    "    </tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBx74UPPYayG"
   },
   "source": [
    "## Information on Taxis\n",
    "\n",
    "<b>Information on taxis:</b><br>\n",
    "* <b>Yellow Taxi:</b> Yellow Medallion Taxicabs<br>\n",
    "These are the famous NYC yellow taxis that provide transportation exclusively through street-hails. The number of taxicabs is limited by a finite number of medallions issued by the TLC. You access this mode of transportation by standing in the street and hailing an available taxi with your hand. The pickups are not pre-arranged.<br><br>\n",
    "* <b>For Hire Vehicles (FHVs)</b><br> \n",
    "FHV transportation is accessed by a pre-arrangement with a dispatcher or limo company. These FHVs are not permitted to pick up passengers via street hails, as those rides are not considered pre-arranged.<br><br>\n",
    "* <b>Green Taxi: Street Hail Livery (SHL)</b><br> \n",
    "The SHL program will allow livery vehicle owners to license and outfit their vehicles with green borough taxi branding, meters, credit card machines, and ultimately the right to accept street hails in addition to pre-arranged rides.<br><br>\n",
    "<b>In this project we are considering only the yellow taxis for the time period of Jan 2015. We have used Jan- 2015 data to make prediction for Jan- 2016 data.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVpsSf-dZE6S"
   },
   "outputs": [],
   "source": [
    "# !pip install -q gpxpy\n",
    "# !pip install -q sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ty2UnwFxZkvN"
   },
   "outputs": [],
   "source": [
    "# !pip install -q sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMGrXvSrYayH"
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import folium\n",
    "import gpxpy.geo\n",
    "from datetime import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve,GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "\n",
    "\n",
    "# from numba import jit, cuda \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GCDjEgwYayJ"
   },
   "outputs": [],
   "source": [
    "# !wget https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-01.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ry_bqKiRfoX9"
   },
   "outputs": [],
   "source": [
    "# !wget https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4F_MfpjibBFf",
    "outputId": "63abaa46-2429-4a6d-dd24-1ce32c434c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML_Project_3.ipynb\t\t  yellow_tripdata_2015-01.csv\r\n",
      "README.md\t\t\t  yellow_tripdata_2016-01.csv\r\n",
      "Taxi_Demand_Prediction_NYC.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLe871faalIm"
   },
   "outputs": [],
   "source": [
    "data_2015 = dd.read_csv(\"yellow_tripdata_2015-01.csv\")\n",
    "# data_2015 = pd.read_csv(\"yellow_tripdata_2015-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "JLit0F3jYayM",
    "outputId": "550a5716-7301-4b5c-d170-bbe3b79faeea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-15 19:05:39</td>\n",
       "      <td>2015-01-15 19:23:42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-73.993896</td>\n",
       "      <td>40.750111</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.974785</td>\n",
       "      <td>40.750618</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:53:28</td>\n",
       "      <td>1</td>\n",
       "      <td>3.30</td>\n",
       "      <td>-74.001648</td>\n",
       "      <td>40.724243</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.994415</td>\n",
       "      <td>40.759109</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:43:41</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-73.963341</td>\n",
       "      <td>40.802788</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.951820</td>\n",
       "      <td>40.824413</td>\n",
       "      <td>2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:35:31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>40.713818</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004326</td>\n",
       "      <td>40.719986</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:52:58</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-73.971176</td>\n",
       "      <td>40.762428</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004181</td>\n",
       "      <td>40.742653</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2015-01-15 19:05:39   2015-01-15 19:23:42                1   \n",
       "1         1  2015-01-10 20:33:38   2015-01-10 20:53:28                1   \n",
       "2         1  2015-01-10 20:33:38   2015-01-10 20:43:41                1   \n",
       "3         1  2015-01-10 20:33:39   2015-01-10 20:35:31                1   \n",
       "4         1  2015-01-10 20:33:39   2015-01-10 20:52:58                1   \n",
       "\n",
       "   trip_distance  pickup_longitude  pickup_latitude  RateCodeID  \\\n",
       "0           1.59        -73.993896        40.750111           1   \n",
       "1           3.30        -74.001648        40.724243           1   \n",
       "2           1.80        -73.963341        40.802788           1   \n",
       "3           0.50        -74.009087        40.713818           1   \n",
       "4           3.00        -73.971176        40.762428           1   \n",
       "\n",
       "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
       "0                  N         -73.974785         40.750618             1   \n",
       "1                  N         -73.994415         40.759109             1   \n",
       "2                  N         -73.951820         40.824413             2   \n",
       "3                  N         -74.004326         40.719986             2   \n",
       "4                  N         -74.004181         40.742653             2   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0         12.0    1.0      0.5        3.25           0.0   \n",
       "1         14.5    0.5      0.5        2.00           0.0   \n",
       "2          9.5    0.5      0.5        0.00           0.0   \n",
       "3          3.5    0.5      0.5        0.00           0.0   \n",
       "4         15.0    0.5      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  \n",
       "0                    0.3         17.05  \n",
       "1                    0.3         17.80  \n",
       "2                    0.3         10.80  \n",
       "3                    0.3          4.80  \n",
       "4                    0.3         16.30  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "XeC7Q8XKYayP",
    "outputId": "81ff30a0-823e-47b3-8e2b-5c07e0667f7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
       "       'passenger_count', 'trip_distance', 'pickup_longitude',\n",
       "       'pickup_latitude', 'RateCodeID', 'store_and_fwd_flag',\n",
       "       'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount',\n",
       "       'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
       "       'improvement_surcharge', 'total_amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CHPOU2pLYayS",
    "outputId": "569ec0a3-b53a-4379-af83-5e36fc264779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns = 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of columns = \"+str(len(data_2015.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6QTdGm1YayU"
   },
   "outputs": [],
   "source": [
    "# data_2015.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A1acjBpBYayX",
    "outputId": "ce4ddf68-e719-4a50-85a5-b7f384bf5a7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<series-..., dtype=int64>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2015[\"passenger_count\"].sum()\n",
    "# data_2015[\"passenger_count\"].sum().visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bpJBM_0zYayZ",
    "outputId": "add0fda6-4668-44dd-dfaa-faea7cdd49bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Passengers in January 2015 = 21437303\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of Passengers in January 2015 = \"+str(data_2015[\"passenger_count\"].sum(axis = 0).compute()))\n",
    "# print(\"Total Number of Passengers in January 2015 = \"+str(data_2015[\"passenger_count\"].sum(axis = 0))) #for pd instead of dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eOiXk_l_Yayb"
   },
   "source": [
    "## Features in Dataset\n",
    "<table>\n",
    "\t<tr>\n",
    "\t\t<th>Field Name</th>\n",
    "\t\t<th>Description</th>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>VendorID</td>\n",
    "\t\t<td>\n",
    "\t\tA code indicating the TPEP provider that provided the record.\n",
    "\t\t<ol>\n",
    "\t\t\t<li>Creative Mobile Technologies</li>\n",
    "\t\t\t<li>VeriFone Inc.</li>\n",
    "\t\t</ol>\n",
    "\t\t</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>tpep_pickup_datetime</td>\n",
    "\t\t<td>The date and time when the meter was engaged.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>tpep_dropoff_datetime</td>\n",
    "\t\t<td>The date and time when the meter was disengaged.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Passenger_count</td>\n",
    "\t\t<td>The number of passengers in the vehicle. This is a driver-entered value.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Trip_distance</td>\n",
    "\t\t<td>The elapsed trip distance in miles reported by the taximeter.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Pickup_longitude</td>\n",
    "\t\t<td>Longitude where the meter was engaged.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Pickup_latitude</td>\n",
    "\t\t<td>Latitude where the meter was engaged.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>RateCodeID</td>\n",
    "\t\t<td>The final rate code in effect at the end of the trip.\n",
    "\t\t<ol>\n",
    "\t\t\t<li> Standard rate </li>\n",
    "\t\t\t<li> JFK </li>\n",
    "\t\t\t<li> Newark </li>\n",
    "\t\t\t<li> Nassau or Westchester</li>\n",
    "\t\t\t<li> Negotiated fare </li>\n",
    "\t\t\t<li> Group ride</li>\n",
    "\t\t</ol>\n",
    "\t\t</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Store_and_fwd_flag</td>\n",
    "\t\t<td>This flag indicates whether the trip record was held in vehicle memory before sending to the vendor,<br\\> aka “store and forward,” because the vehicle did not have a connection to the server.\n",
    "\t\t<br\\>Y= store and forward trip\n",
    "\t\t<br\\>N= not a store and forward trip\n",
    "\t\t</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "\t\t<td>Dropoff_longitude</td>\n",
    "\t\t<td>Longitude where the meter was disengaged.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Dropoff_ latitude</td>\n",
    "\t\t<td>Latitude where the meter was disengaged.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Payment_type</td>\n",
    "\t\t<td>A numeric code signifying how the passenger paid for the trip.\n",
    "\t\t<ol>\n",
    "\t\t\t<li> Credit card </li>\n",
    "\t\t\t<li> Cash </li>\n",
    "\t\t\t<li> No charge </li>\n",
    "\t\t\t<li> Dispute</li>\n",
    "\t\t\t<li> Unknown </li>\n",
    "\t\t\t<li> Voided trip</li>\n",
    "\t\t</ol>\n",
    "\t\t</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Fare_amount</td>\n",
    "\t\t<td>The time-and-distance fare calculated by the meter.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Extra</td>\n",
    "\t\t<td>Miscellaneous extras and surcharges. Currently, this only includes. the $0.50 and $1 rush hour and overnight charges.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>MTA_tax</td>\n",
    "\t\t<td>0.50 MTA tax that is automatically triggered based on the metered rate in use.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Improvement_surcharge</td>\n",
    "\t\t<td>0.30 improvement surcharge assessed trips at the flag drop. the improvement surcharge began being levied in 2015.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Tip_amount</td>\n",
    "\t\t<td>Tip amount – This field is automatically populated for credit card tips.Cash tips are not included.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Tolls_amount</td>\n",
    "\t\t<td>Total amount of all tolls paid in trip.</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Total_amount</td>\n",
    "\t\t<td>The total amount charged to passengers. Does not include cash tips.</td>\n",
    "\t</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "2i7lRYScYayc",
    "outputId": "db37ac90-c908-48a5-ef0c-20641aeb668e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nan values = VendorID                 0\n",
      "tpep_pickup_datetime     0\n",
      "tpep_dropoff_datetime    0\n",
      "passenger_count          0\n",
      "trip_distance            0\n",
      "pickup_longitude         0\n",
      "pickup_latitude          0\n",
      "RateCodeID               0\n",
      "store_and_fwd_flag       0\n",
      "dropoff_longitude        0\n",
      "dropoff_latitude         0\n",
      "payment_type             0\n",
      "fare_amount              0\n",
      "extra                    0\n",
      "mta_tax                  0\n",
      "tip_amount               0\n",
      "tolls_amount             0\n",
      "improvement_surcharge    3\n",
      "total_amount             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nan values = {}\".format(data_2015.isnull().sum().compute()))\n",
    "# print(\"Number of nan values = {}\".format(data_2015.isnull().sum())) #for pd instead of dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aer4sWEqYaye"
   },
   "source": [
    "Since we will not be using \"improvement_surcharge\" in our task, so nan values will not affect us. So, we are leaving it as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I99QDXT1Yayf"
   },
   "source": [
    "## Problem Formulation: Time Series Forecasting\n",
    "\n",
    "Given a region and a 10min interval, we have to predict pickups.\n",
    "* (a): How to break up the NYC into regions?\n",
    "* (b): Every region of NYC has to be broken up into 10min  interval.<br>\n",
    "\n",
    "Now, every row pickup has longitude and latitude in it, and we will use this to define pickup regions. \n",
    "We already know, about the pickup at time 't', we will predict the pickup at time 't+1' in the same region. Hence, this problem can be thought of as a 'Time Series Prediction' problem. It is a special case of regression problems. In short, we will use the data at time 't' to predict for time 't+1'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L8lBioAWYayg"
   },
   "source": [
    "## Performance Metric\n",
    "* 1) Mean Absolute Percentage Error(MAPE)\n",
    "* 2) Mean Squared Error(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2lID_kEYayi"
   },
   "source": [
    "# 2. Univariate Analysis & Data Cleaning\n",
    "Here, we will do the univariate analysis of the data and remove all the erroneous/outlier points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJKMVcKmYayk"
   },
   "source": [
    "## Latitude and Longitude\n",
    "It is inferred from the source: https://www.flickr.com/places/info/2459115 that New York is bounded by the location coordinates(latitude, longitude) - (40.5774, -74.15) & (40.9176,-73.7004) hence any coordinates--including both pickups and drop-offs--not within these coordinates are not considered by us as we are only concerned with pickups which originate within New York. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq3fwjGGYayk"
   },
   "source": [
    "### Pickup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Wi0LgATYayl",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #detecting the pickups latitude and longitudes which are outside NYC.\n",
    "# outside_NYC = data_2015[((data_2015.pickup_latitude <= 40.5774) | (data_2015.pickup_longitude <= -74.15) | (data_2015.pickup_latitude >= 40.9176) | (data_2015.pickup_longitude >= -73.7004))]\n",
    "# #latitude at equator is 0. Above equator latitude increases and becomes 90 at north pole. Below equator latitude decreases and\n",
    "# #is negative and becomes -90 at south pole.\n",
    "# #Longitude is 0 at United Kingdom(UK). To the right of UK, longitude increases positively and to the left of UK longitude decrease\n",
    "# #and is negative.\n",
    "\n",
    "# m = folium.Map(location = [40.5774, -73.7004], tiles = \"Stamen Toner\")\n",
    "\n",
    "# outside_pickups = outside_NYC.head(25000)\n",
    "\n",
    "# for i,j in outside_pickups.iterrows():\n",
    "#     if j[\"pickup_latitude\"] != 0:\n",
    "#         folium.Marker([j[\"pickup_latitude\"], j[\"pickup_longitude\"]]).add_to(m)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mL31nTSoYayo"
   },
   "source": [
    "<b>Observation:</b>As you can see in the above map, there are many erroneous data points, which are in some other state, and some of them are even in Atlantic Ocean. All these erroneous data points will also be removed as a part of data cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2dzwaFQYayo"
   },
   "source": [
    "### Dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0kTwS_BYayp"
   },
   "outputs": [],
   "source": [
    "# #detecting the dropoff latitude and longitudes which are outside NYC.\n",
    "# outside_NYC = data_2015[((data_2015.dropoff_latitude <= 40.5774) | (data_2015.dropoff_longitude <= -74.15) | (data_2015.dropoff_latitude >= 40.9176) | (data_2015.dropoff_longitude >= -73.7004))]\n",
    "# #latitude at equator is 0. Above equator latitude increases and becomes 90 at north pole. Below equator latitude decreases and\n",
    "# #is negative and becomes -90 at south pole.\n",
    "# #Longitude is 0 at United Kingdom(UK). To the right of UK, longitude increases positively and to the left of UK longitude decrease\n",
    "# #and is negative.\n",
    "\n",
    "# m = folium.Map(location = [40.5774, -73.7004], tiles = \"Stamen Toner\")\n",
    "\n",
    "# outside_dropoff = outside_NYC.head(25000)\n",
    "\n",
    "# for i,j in outside_dropoff.iterrows():\n",
    "#     if j[\"dropoff_latitude\"] != 0:\n",
    "#         folium.Marker([j[\"dropoff_latitude\"], j[\"dropoff_longitude\"]]).add_to(m)\n",
    "# m\n",
    "# #documentation of folium: https://python-visualization.github.io/folium/docs-v0.6.0/quickstart.html#Getting-Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZiDc4-jHYays"
   },
   "source": [
    "<b>Observation:</b>As you can see in the above map, there are many erroneous data points, which are in some other state, and some of them are even in Atlantic Ocean. All these erroneous data points will also be removed as a part of data cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MX0dEyjLYays"
   },
   "source": [
    "### Creating new Dataframe with Trip duration and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLvz5tRbYayt"
   },
   "outputs": [],
   "source": [
    "def timeToUnix(t):\n",
    "    #we have a time in the format \"YYYY-MM-DD HH:MM:SS\", which is a string\n",
    "    change = datetime.strptime(t, \"%Y-%m-%d %H:%M:%S\") #this will convert the String time into datetime format\n",
    "    t_tuple = change.timetuple() #this will convert the datetime formatted time into structured time\n",
    "    return time.mktime(t_tuple) + 3600  #this will convert structured time into unix-time.\n",
    "    #Now why, I have added 3600 in the above unix times. NOW, UNIX TIMESTAMP MEANS HOW MANY SECONDS HAVE ELAPSED SINCE 1 JAN 1970\n",
    "    #(EPOCH) CALCULATED FROM THE REFERENCE OF GMT. I HAVE MADE THIS PROJECT IN GERMANY WHICH IS 1HR/3600SECS AHEAD OF GMT TIME, \n",
    "    #AND HERE \"time.mktime()\" FUNCTION RETURNS UNIX TIMESTAMP FROM THE REFERENCE OF LOCAL TIME. SO, THEREFORE, IN ORDER TO \n",
    "    #COMPENSATE FOR 1HR AHEAD, \"time.mktime\" SUBTRACTED 3600 SECONDS MEANS 1HR FROM UNIX TIME STAMP IN ORDER TO CATER TO \n",
    "    #LOCAL TIME. SO, THEREFORE, IF WE WANT OUR UNIX TIME TO BE EXACTLY EQUAL TO GMT TIME, WE HAVE TO ADD 3600 SECONDS \n",
    "    #MEANS 1HR TO UNIX TIME. lET SAY AT 12:00AM ON 1st JAN 1970, TIME ELAPSED AT GMT IS 0, THE TIME ELAPSED IN GERMANY IS \n",
    "    #3600SEC. NOW ON 1st JAN 2015, ELASPED SECONDS AT GMT IS 'X', SO THE EQUIVALENT ELAPSED SECONDS IN GERMANY WILL BE X+3600. \n",
    "    #NOW \"time.mktime()\" SUBTRACT THIS 3600 EXTRA IN GERMAN TIME WHICH WE HAVE TO ADD IN ORDER TO MAKE IT EQUAL TO GMT.\n",
    "\n",
    "def dfWithTripTimes(df):\n",
    "    startTime = datetime.now()\n",
    "    duration = df[[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]].compute()\n",
    "    #duration = df[[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]] #for pd instead of dd\n",
    "\n",
    "    pickup_time = [timeToUnix(pkup) for pkup in duration[\"tpep_pickup_datetime\"].values]\n",
    "    dropoff_time = [timeToUnix(drpof) for drpof in duration[\"tpep_dropoff_datetime\"].values]\n",
    "#     trip_duration = []\n",
    "#     for xy in zip(dropoff_time, pickup_time):\n",
    "#         trip_duration.append(xy[0] - xy[1])\n",
    "    trip_duration = (np.array(dropoff_time) - np.array(pickup_time))/float(60)  #trip duration in minutes\n",
    "    \n",
    "    NewFrame = df[['passenger_count','trip_distance','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','total_amount']].compute()\n",
    "#     NewFrame = df[['passenger_count','trip_distance','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','total_amount']] #for pd instead of dd\n",
    "    NewFrame[\"trip_duration\"] = trip_duration\n",
    "    NewFrame[\"pickup_time\"] = pickup_time\n",
    "    NewFrame[\"speed\"] = (NewFrame[\"trip_distance\"]/NewFrame[\"trip_duration\"])*60  #speed in miles/hr\n",
    "    \n",
    "    print(\"Time taken for creation of dataframe is {}\".format(datetime.now() - startTime))\n",
    "    return NewFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ts040oNcYayv",
    "outputId": "187c68ce-59d2-4fc3-b789-2accd75c2cbf"
   },
   "outputs": [],
   "source": [
    "new_frame = dfWithTripTimes(data_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSp4PufHYayx"
   },
   "source": [
    "## Trip Durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxblmT1sYayy"
   },
   "source": [
    "### According to NYC Taxi and Limousine Commission regulations, the maximum allowed trip duration in a 24hrs interval is 12 hrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "VUGq1swFYayz",
    "outputId": "4c8ea674-0773-4e75-cca3-e8089a04ad36"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (10,6))\n",
    "# sns.boxplot(\"trip_duration\", data = new_frame, orient = \"v\")\n",
    "# plt.tick_params(labelsize = 20)\n",
    "# plt.ylabel(\"Trip Duration(minutes)\", fontsize = 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A52yA8RWYay1"
   },
   "outputs": [],
   "source": [
    "quantile_tripDuration = new_frame.trip_duration.quantile(np.round(np.arange(0.00, 1.01, 0.01), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "dTBjqIfmYay5",
    "outputId": "e945ab2c-3cba-456e-9882-c5edac2656e4"
   },
   "outputs": [],
   "source": [
    "qValues = np.round(np.arange(0.00, 1.01, 0.1), 2)\n",
    "for i in qValues:\n",
    "    print(\"{}th percentile value of Trip Duration is {}min\".format((int(i*100)), quantile_tripDuration[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "_dIjuRbjYay-",
    "outputId": "f48a84a3-20a3-4621-bbf0-58234781e3ed"
   },
   "outputs": [],
   "source": [
    "qValues = np.round(np.arange(0.90, 1.01, 0.01), 2)\n",
    "for i in qValues:\n",
    "    print(\"{} percentile value of Trip Duration is {}min\".format((int(i*100)), quantile_tripDuration[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ri9eR4cJYazA"
   },
   "source": [
    "<b>Observations:</b>Here, 0th percentile value of trip duration is negative(weird), also 99th percentile value of trip duration is 46.75min, but 100th percentile value is 548555.633min(weird). 0th and 100th percentile values are certainly an erroneous points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKlRfUCqYazA"
   },
   "outputs": [],
   "source": [
    "new_frame_cleaned = new_frame[(new_frame.trip_duration>1) & (new_frame.trip_duration<720)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49fbtQPIYazC"
   },
   "source": [
    "#### Plot after removing outliers and erroneous points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "XZA83YNZYazD",
    "outputId": "f3c8e152-d965-4737-a493-7a0df24a5e31"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (10,6))\n",
    "# sns.boxplot(\"trip_duration\", data = new_frame_cleaned, orient = \"v\")\n",
    "# plt.ylim(ymin = 1, ymax = 750)\n",
    "# plt.tick_params(labelsize = 20)\n",
    "# plt.ylabel(\"Trip Duration(minutes)\", fontsize = 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "6l5YeJOsYazG",
    "outputId": "f6eba8e3-4091-4868-b1aa-9d55bc7d9533"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "sns.kdeplot(new_frame_cleaned[\"trip_duration\"].values, shade = True, cumulative = False)\n",
    "plt.tick_params(labelsize = 20)\n",
    "plt.xlabel(\"Trip Duration\", fontsize = 20)\n",
    "plt.title(\"PDF of Trip Duration\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMY1rIUeYazI"
   },
   "source": [
    "<b>Observation:</b> Above PDF plot shows that almost all of the trip durations are very less and approximately less than 100, extremely few trip durations are above 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBBC4QuRYazJ"
   },
   "source": [
    "## Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djqvumpjYazK"
   },
   "outputs": [],
   "source": [
    "def changingLabels(num):\n",
    "    if num < 10**3:\n",
    "        return num\n",
    "    elif num>=10**3 and num < 10**6:\n",
    "        return str(num/10**3)+\"k\"\n",
    "    elif num>=10**6 and num < 10**9:\n",
    "        return str(num/10**6) + \"M\"\n",
    "    else:\n",
    "        return str(num/10**9) + \"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "EVbCiZ3NYazM",
    "outputId": "7332db9d-b86d-407b-dd14-da5301edd2d9"
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (10,6))\n",
    "# ax = sns.boxplot(\"speed\", data = new_frame_cleaned, orient = \"v\")\n",
    "\n",
    "# ax.set_yticklabels([changingLabels(num) for num in ax.get_yticks()])\n",
    "# plt.tick_params(labelsize = 20)\n",
    "# plt.ylabel(\"Speed(Miles/hr) in Millions\", fontsize = 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cu2csZAmYazO"
   },
   "outputs": [],
   "source": [
    "quantile_speed = new_frame_cleaned.speed.quantile(np.round(np.arange(0.00, 1.01, 0.01), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "LujtQ4d7YazQ",
    "outputId": "e3263c56-f7bc-47f6-a7d1-17e97241908e"
   },
   "outputs": [],
   "source": [
    "qValues = np.round(np.arange(0.00, 1.01, 0.1), 4)\n",
    "for i in qValues:\n",
    "    print(\"{}th percentile value of speed is {}miles/hr\".format(int(i*100), quantile_speed[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "UwuKKfaUYazU",
    "outputId": "d8ed83cb-feff-4d04-a4dc-1b233a05510a"
   },
   "outputs": [],
   "source": [
    "qValues = np.round(np.arange(0.91, 1.01, 0.01), 3)\n",
    "for i in qValues:\n",
    "    print(\"{} percentile value of speed is {}miles/hr\".format(int(i*100), quantile_speed[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "KqXPROxjYazX",
    "outputId": "c8c923f2-6218-4570-cee0-51452e7b484a"
   },
   "outputs": [],
   "source": [
    "qValues = np.round(np.arange(0.991, 1.001, 0.001), 4)\n",
    "quantile_speed = new_frame_cleaned.speed.quantile(qValues)\n",
    "for i in qValues:\n",
    "    print(\"{} percentile value of speed is {}miles/hr\".format((i*100), quantile_speed[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ycUHqiPYazZ"
   },
   "source": [
    "<b>Observations:</b> Here, 100th percentile value of a speed is 192 Million miles/hr which is (BIZZARE). Furthermore, 99.9th percentile value of speed is 45.31miles/hr. So, we are removing all the data points where speed is greater than 45.31miles/hr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJa4jv0OYazZ"
   },
   "outputs": [],
   "source": [
    "new_frame_cleaned = new_frame_cleaned[(new_frame_cleaned.speed>0) & (new_frame_cleaned.speed<45.31)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lC2snNshYazb"
   },
   "source": [
    "#### Box plot of speed after removing outliers and erroneous points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "Xs5FctviYazc",
    "outputId": "66022005-7732-4262-a2f2-ac0783e52d7c"
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (10,6))\n",
    "# ax = sns.boxplot(\"speed\", data = new_frame_cleaned, orient = \"v\")\n",
    "\n",
    "# plt.tick_params(labelsize = 20)\n",
    "# plt.ylabel(\"Speed(Miles/hr)\", fontsize = 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JQHGu07-Yaze",
    "outputId": "2b826045-b2b2-4537-e7d0-a8a8c341d363"
   },
   "outputs": [],
   "source": [
    "Average_speed = sum(new_frame_cleaned.speed)/len(new_frame_cleaned.speed)\n",
    "print(\"Average Speed of Taxis around NYC = \"+str(Average_speed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hDdoBwmEYazg",
    "outputId": "8532c248-f32a-44e1-dfa1-ce34d1f68c9a"
   },
   "outputs": [],
   "source": [
    "print(\"Speed of Taxis around NYC per 10 minutes = \"+str(Average_speed/6)+\" per 10 minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DB-_UcXoYazi"
   },
   "source": [
    "### The avg speed in Newyork speed is 12.45miles/hr, so a cab driver can travel 2 miles per 10min on avg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2xI9pXCpYazj"
   },
   "source": [
    "## Trip Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "oD7PjrGIYazm",
    "outputId": "a08fbd74-e2ab-4c01-e2c2-b475045330e6"
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (10,6))\n",
    "# ax = sns.boxplot(\"trip_distance\", data = new_frame_cleaned, orient = \"v\")\n",
    "\n",
    "# plt.tick_params(labelsize = 20)\n",
    "# plt.ylabel(\"Trip Distance(Miles)\", fontsize = 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMHDonSXYazo"
   },
   "outputs": [],
   "source": [
    "quantile_tripDistance = new_frame_cleaned.trip_distance.quantile(np.round(np.arange(0.00, 1.01, 0.01), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "GEQoZVw6Yazq",
    "outputId": "26df419f-63c4-46fc-a196-c9185bfaa7db"
   },
   "outputs": [],
   "source": [
    "qValues = np.round(np.arange(0.00, 1.01, 0.1), 2)\n",
    "for i in qValues:\n",
    "    print(\"{}th percentile value of trip distance is {}miles\".format(int(i*100), quantile_tripDistance[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "i7yP4-uLYazs",
    "outputId": "4dd4254a-f131-4917-acd6-beccccc56120"
   },
   "outputs": [],
   "source": [
    "qValues = np.round(np.arange(0.91, 1.01, 0.01), 3)\n",
    "for i in qValues:\n",
    "    print(\"{} percentile value of trip distance is {}miles\".format(int(i*100), quantile_tripDistance[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "X_apoY9LYazv",
    "outputId": "7523953b-9c57-4880-9db0-83de0f1aab0c"
   },
   "outputs": [],
   "source": [
    "quantile_tripDistance = new_frame_cleaned.trip_distance.quantile(np.round(np.arange(0.991, 1.001, 0.001), 4))\n",
    "qValues = np.round(np.arange(0.991, 1.001, 0.001), 3)\n",
    "for i in qValues:\n",
    "    print(\"{} percentile value of trip distance is {}miles\".format((i*100), quantile_tripDistance[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uB-eswg8Yazx"
   },
   "source": [
    "<b>Observation:</b> Here, 99.9th percentile of trip distance is 22.58miles, however, 100th percentile value is 258.9miles, which is very high. So, we are removing all the data points where trip distance is greater than 23miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46PK6TfTYazy"
   },
   "outputs": [],
   "source": [
    "new_frame_cleaned = new_frame_cleaned[(new_frame_cleaned.trip_distance>0) & (new_frame_cleaned.trip_distance<23)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "mJFSsLd5Yazz",
    "outputId": "f2ce85c6-8be7-4349-9a70-3a4b91fa20df"
   },
   "outputs": [],
   "source": [
    "# #plot of trip distance after removing outlier points\n",
    "# fig = plt.figure(figsize = (10,6))\n",
    "# ax = sns.boxplot(\"trip_distance\", data = new_frame_cleaned, orient = \"v\")\n",
    "\n",
    "# plt.tick_params(labelsize = 20)\n",
    "# plt.ylabel(\"Trip Distance(Miles)\", fontsize = 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nj8OPjqMYaz1"
   },
   "source": [
    "## Total Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "eMsi4EqMYaz2",
    "outputId": "9123ac98-f68b-4810-b92c-1fd646e8f540"
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (10,6))\n",
    "# ax = sns.boxplot(\"total_amount\", data = new_frame_cleaned, orient = \"v\")\n",
    "\n",
    "# plt.tick_params(labelsize = 20)\n",
    "# plt.ylabel(\"Trip Fare\", fontsize = 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6XQIi8WYaz4"
   },
   "outputs": [],
   "source": [
    "quantile_totalAmount = new_frame_cleaned.total_amount.quantile(np.round(np.arange(0.00, 1.01, 0.01), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "PSW6yUIrYaz5",
    "outputId": "625b9742-96f4-4f4f-d086-9b0e8847b303"
   },
   "outputs": [],
   "source": [
    "qValues = np.round(np.arange(0.00, 1.01, 0.1), 2)\n",
    "for i in qValues:\n",
    "    print(\"{}th percentile value of trip fare is {}\".format(int(i*100), quantile_totalAmount[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "7Tgj1tezYaz9",
    "outputId": "60560be1-6768-4b2e-9faa-4463a3adc37e"
   },
   "outputs": [],
   "source": [
    "qValues = np.round(np.arange(0.91, 1.01, 0.01), 3)\n",
    "for i in qValues:\n",
    "    print(\"{} percentile value of trip fare is {}\".format(int(i*100), quantile_totalAmount[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "dvZxTTjPYa0A",
    "outputId": "7558852d-cfb8-4b14-abe7-d1bc29fe8541"
   },
   "outputs": [],
   "source": [
    "quantile_totalAmount = new_frame_cleaned.total_amount.quantile(np.round(np.arange(0.991, 1.001, 0.001), 3))\n",
    "qValues = np.round(np.arange(0.991, 1.001, 0.001), 3)\n",
    "for i in qValues:\n",
    "    print(\"{} percentile value of trip fare is {}\".format((i*100), quantile_totalAmount[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Pjm7ci2Ya0C"
   },
   "source": [
    "<b>Observation:</b>Here, 99.9th percentile fare amount of a trip is 86.6. However, 100th percentile of a fare amount is 3Million which is bizzare. Therefore, we have removedall the data points where fare amount is more than 99.9th percentile value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j1ks5QkIYa0D"
   },
   "outputs": [],
   "source": [
    "new_frame_cleaned = new_frame_cleaned[(new_frame_cleaned.total_amount>0) & (new_frame_cleaned.total_amount<86.6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "01vGfbA1Ya0F",
    "outputId": "22b89bb4-a1ae-4b04-a521-fff10d7b020b"
   },
   "outputs": [],
   "source": [
    "# #plot of fare amount after removing outliers and erroneous points\n",
    "# fig = plt.figure(figsize = (10,6))\n",
    "# ax = sns.boxplot(\"total_amount\", data = new_frame_cleaned, orient = \"v\")\n",
    "\n",
    "# plt.tick_params(labelsize = 20)\n",
    "# plt.ylabel(\"Trip Fare\", fontsize = 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "our9tR44Ya0I"
   },
   "source": [
    "## Removing all the data points where pickup and drop-off are outside NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pWO_BG0Ya0J"
   },
   "source": [
    "### Removing Pickups outside NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyPmBVU-Ya0K"
   },
   "outputs": [],
   "source": [
    "new_frame_cleaned = new_frame_cleaned[(((new_frame_cleaned.pickup_latitude >= 40.5774) & (new_frame_cleaned.pickup_latitude <= 40.9176)) & ((new_frame_cleaned.pickup_longitude >= -74.15) & (new_frame_cleaned.pickup_longitude <= -73.7004)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KkToSz6-Ya0M",
    "outputId": "8a2e8278-1569-4ba5-e20c-6440d3d7a2c5"
   },
   "outputs": [],
   "source": [
    "# m = folium.Map(location = [40.9176, -73.7004], tiles = \"Stamen Toner\")\n",
    "\n",
    "# pickups_within_NYC = new_frame_cleaned.sample(n = 500)\n",
    "\n",
    "# for i,j in pickups_within_NYC.iterrows():\n",
    "#     folium.Marker([j[\"pickup_latitude\"], j[\"pickup_longitude\"]]).add_to(m)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWmFQ5yFYa0P"
   },
   "source": [
    "<b>Observation:</b> Most of the pickups are concentrated in and around Manhattan district of New York"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMzuG2gtYa0P"
   },
   "source": [
    "### Removing Dropoffs outside NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NN6NlGSzYa0Q"
   },
   "outputs": [],
   "source": [
    "new_frame_cleaned = new_frame_cleaned[(((new_frame_cleaned.dropoff_latitude >= 40.5774) & (new_frame_cleaned.dropoff_latitude <= 40.9176)) & ((new_frame_cleaned.dropoff_longitude >= -74.15) & (new_frame_cleaned.dropoff_longitude <= -73.7004)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mN0u70z5Ya0R",
    "outputId": "a7e887cc-1146-4fe8-a551-e19c84550b7f"
   },
   "outputs": [],
   "source": [
    "# m = folium.Map(location = [40.9176, -73.7004], tiles = \"Stamen Toner\")\n",
    "\n",
    "# dropoff_within_NYC = new_frame_cleaned.sample(n = 500)\n",
    "\n",
    "# for i,j in dropoff_within_NYC.iterrows():\n",
    "#     folium.Marker([j[\"dropoff_latitude\"], j[\"dropoff_longitude\"]]).add_to(m)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yH_TrP8_Ya0T"
   },
   "source": [
    "<b>Observation:</b> Most of the dropoffs are concentrated in and around Manhattan district of New York"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vIK7yxfiYa0U"
   },
   "source": [
    "### Fraction of points left after removing all the erroneous points and outlier points. Points where pickups and dropoffs are outside of NYC are also removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FzybFeJWYa0V",
    "outputId": "bb8b8ee6-7ba6-42a2-e7ab-b66e717909be"
   },
   "outputs": [],
   "source": [
    "print(\"Fraction of cleaned points\",str(new_frame_cleaned.shape[0]/new_frame.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ERrXMeAAYa0W",
    "outputId": "1ba6f34b-9baa-4a06-ab68-6992eb341847"
   },
   "outputs": [],
   "source": [
    "print(\"Total number of outliers and erroneous points removed = \",str(new_frame.shape[0] - new_frame_cleaned.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DLz-6mycYa0Y"
   },
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_8n46AbYa0b"
   },
   "source": [
    "## Clustering/Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCSeQ7bvYa0b"
   },
   "outputs": [],
   "source": [
    "coord = new_frame_cleaned[[\"pickup_latitude\", \"pickup_longitude\"]].values\n",
    "neighbors = []\n",
    "\n",
    "def min_distance(regionCenters, totalClusters):\n",
    "    good_points = 0\n",
    "    bad_points = 0\n",
    "    less_dist = []\n",
    "    more_dist = []\n",
    "    min_distance = 100000  #any big number can be given here\n",
    "    for i in range(totalClusters):\n",
    "        good_points = 0\n",
    "        bad_points = 0\n",
    "        for j in range(totalClusters):\n",
    "            if j != i:\n",
    "                distance = gpxpy.geo.haversine_distance(latitude_1 = regionCenters[i][0], longitude_1 = regionCenters[i][1], latitude_2 = regionCenters[j][0], longitude_2 = regionCenters[j][1])\n",
    "                #you can check the documentation of above \"gpxpy.geo.haversine_distance\" at \"https://github.com/tkrajina/gpxpy/blob/master/gpxpy/geo.py\"\n",
    "                #\"gpxpy.geo.haversine_distance\" gives distance between two latitudes and longitudes in meters. So, we have to convert it into miles.\n",
    "                distance = distance/(1.60934*1000)   #distance from meters to miles\n",
    "                min_distance = min(min_distance, distance) #it will return minimum of \"min_distance, distance\".\n",
    "                if distance < 2:\n",
    "                    good_points += 1\n",
    "                else:\n",
    "                    bad_points += 1\n",
    "        less_dist.append(good_points)\n",
    "        more_dist.append(bad_points)\n",
    "    print(\"On choosing a cluster size of {}\".format(totalClusters))\n",
    "    print(\"Avg. Number clusters within vicinity where inter cluster distance < 2 miles is {}\".format(np.ceil(sum(less_dist)/len(less_dist))))\n",
    "    print(\"Avg. Number clusters outside of vicinity where inter cluster distance > 2 miles is {}\".format(np.ceil(sum(more_dist)/len(more_dist))))\n",
    "    print(\"Minimum distance between any two clusters = {}\".format(min_distance))\n",
    "    print(\"-\"*10)\n",
    "            \n",
    "def makingRegions(noOfRegions):\n",
    "    regions = MiniBatchKMeans(n_clusters = noOfRegions, batch_size = 10000).fit(coord)\n",
    "    regionCenters = regions.cluster_centers_ \n",
    "    totalClusters = len(regionCenters)\n",
    "    return regionCenters, totalClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "cQ4j_KHqYa0d",
    "outputId": "f96fb187-7aad-4620-cb59-5544bab3f46d"
   },
   "outputs": [],
   "source": [
    "startTime = datetime.now()\n",
    "for i in range(10, 100, 10):\n",
    "    regionCenters, totalClusters = makingRegions(i)\n",
    "    min_distance(regionCenters, totalClusters)\n",
    "print(\"Time taken = \"+str(datetime.now() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJltlrztYa0e"
   },
   "source": [
    "#### We want the minimum inter cluster distance between any two clusters to be less than 0.5miles and when number of clusters are 30 then this condition is almost meeting. Therefore, we are considering number of clusters to be 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePQAcrF4Ya0f"
   },
   "source": [
    "The MiniBatchKMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G10NMbP3Ya0f"
   },
   "outputs": [],
   "source": [
    "coord = new_frame_cleaned[[\"pickup_latitude\", \"pickup_longitude\"]].values\n",
    "regions = MiniBatchKMeans(n_clusters = 30, batch_size = 10000).fit(coord)\n",
    "new_frame_cleaned[\"pickup_cluster\"] = regions.predict(new_frame_cleaned[[\"pickup_latitude\", \"pickup_longitude\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "LXgwOtakYa0h",
    "outputId": "f192dd33-efc6-4d38-833d-84e1905dd4a3"
   },
   "outputs": [],
   "source": [
    "new_frame_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt6yhFcGYa0j"
   },
   "source": [
    "#### Plotting cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZPKchMIxYa0j",
    "outputId": "46b97749-abd9-4b3c-a01e-ec3963e7326b"
   },
   "outputs": [],
   "source": [
    "# centerOfRegions = regions.cluster_centers_\n",
    "# noOfClusters = len(centerOfRegions)\n",
    "# m = folium.Map(location = [40.9176, -73.7004], tiles = \"Stamen Toner\")\n",
    "\n",
    "# for i in range(noOfClusters):\n",
    "#     folium.Marker([centerOfRegions[i][0], centerOfRegions[i][1]], popup = (str(np.round(centerOfRegions[i][0], 2))+\", \"+str(np.round(centerOfRegions[i][1], 2)))).add_to(m)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nJPFXlnAYa0m"
   },
   "source": [
    "#### Plotting Regions in NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "Uy2tPMZ3Ya0n",
    "outputId": "ebc493fa-1b9f-4ab6-eb18-92f725ef489c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NYC_latitude_range = (40.5774, 40.9176)\n",
    "NYC_Longitude_range = (-74.15, -73.7004)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1.5,1.5])\n",
    "ax.scatter(x = new_frame_cleaned.pickup_longitude.values[:70000], y = new_frame_cleaned.pickup_latitude.values[:70000], c = new_frame_cleaned.pickup_cluster.values[:70000], cmap = \"Paired\", s = 5)\n",
    "ax.set_xlim(-74.10, -73.72)\n",
    "ax.set_ylim(40.5774, 40.9176)\n",
    "ax.set_title(\"Regions in New York City\")\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "plt.show()\n",
    "#Longitude values vary from left to right i.e., horizontally\n",
    "#Latitude values vary from top to bottom means i.e., vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EsOBbcPHYa0p"
   },
   "source": [
    "## Time Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcjoNGmjYa0p"
   },
   "outputs": [],
   "source": [
    "# 1420070400 : 2015-01-01 00:00:00   (Equivalent unix time)\n",
    "# 1451606400 : 2016-01-01 00:00:00   (Equivalent unix time)\n",
    "\n",
    "def pickup_10min_bins(dataframe, month, year):\n",
    "    pickupTime = dataframe[\"pickup_time\"].values\n",
    "    unixTime = [1420070400, 1451606400]\n",
    "    unix_year = unixTime[year-2015]\n",
    "    time_10min_bin = [int((i - unix_year)/600) for i in pickupTime]\n",
    "    dataframe[\"time_bin\"] = np.array(time_10min_bin)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bk35PaxHYa0r"
   },
   "outputs": [],
   "source": [
    "# In the above function we are just subtracting the pickup time from the unix time of 12:00AM 1st Jan 2015, and after that we \n",
    "# are dividing that with 600 in order to make a 10minute bin, that's it.\n",
    "#For Jan 2016 data we are just subtracting its corresponding pickup time from the unix time of 12:00AM 1st Jan 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDUt66_WYa0t"
   },
   "outputs": [],
   "source": [
    "jan_2015_data = pickup_10min_bins(new_frame_cleaned, 1, 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "bKu5ozCQYa0u",
    "outputId": "160ad000-49d1-4f6e-9a2e-99733146ea0c"
   },
   "outputs": [],
   "source": [
    "jan_2015_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-SNR57lJYa0y",
    "outputId": "e1e8c8e7-3ce0-4cca-e559-7bc5f67ddbea"
   },
   "outputs": [],
   "source": [
    "print(\"There should be ((24*60)/10)*31 unique 10 minute time bins for the month of January 2015: \", str(len(np.unique(jan_2015_data[\"time_bin\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-VaIj4sYa0z"
   },
   "outputs": [],
   "source": [
    "jan_2015_timeBin_groupBy = jan_2015_data[[\"pickup_cluster\", \"time_bin\", \"trip_distance\"]].groupby(by = [\"pickup_cluster\", \"time_bin\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "QIAypSHRYa01",
    "outputId": "f56512b8-de73-4208-ad30-819f0c63427f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jan_2015_timeBin_groupBy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntvadQLOYa02"
   },
   "source": [
    "## Data Preparation for Januray 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "lHoWeRIzYa03",
    "outputId": "d6d13601-2b77-4558-c785-e824bd7aa277"
   },
   "outputs": [],
   "source": [
    "# Up till now we cleaned data and prepared data for the month of Jan 2015.\n",
    "\n",
    "# now doing the same operations for the month of Jan 2016.\n",
    "\n",
    "# 1. Get the dataframe which includes only required colums.\n",
    "# 2. Add trip_duration, speed, unix time stamp of pickup_time.\n",
    "# 4. Remove the outliers based on trip_duration, speed, trip_distance, total_amount.\n",
    "# 5. Remove all the points where pickup and dropoff are outside of New York City area.\n",
    "# 6. Add pickup_cluster to each data point.\n",
    "# 7. Add time_bin (index of 10min intravel to which that trip belongs to).\n",
    "# 8. Group by data, based on 'pickup_cluster' and 'time_bin'\n",
    "startTime = datetime.now()\n",
    "frame_2016 = dd.read_csv(\"yellow_tripdata_2016-01.csv\")\n",
    "# frame_2016 = pd.read_csv(\"yellow_tripdata_2016-01.csv\")\n",
    "\n",
    "print(\"PREPARATION OF JANUARY 2016 DATA.\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Number of columns = \"+str(len(frame_2016.columns)))\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame2 = dfWithTripTimes(frame_2016)\n",
    "print(\"New Frame for Jan 2016 creation done\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame2[(new_frame2.trip_duration>1) & (new_frame2.trip_duration<720)]\n",
    "print(\"Trip Duration Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame_cleaned2[(new_frame_cleaned2.speed>0) & (new_frame_cleaned2.speed<45.31)]\n",
    "print(\"Speed Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame_cleaned2[(new_frame_cleaned2.trip_distance>0) & (new_frame_cleaned2.trip_distance<23)]\n",
    "print(\"Trip Distance Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame_cleaned2[(new_frame_cleaned2.total_amount>0) & (new_frame_cleaned2.total_amount<86.6)]\n",
    "print(\"Total Amount Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame_cleaned2[(((new_frame_cleaned2.pickup_latitude >= 40.5774) & (new_frame_cleaned2.pickup_latitude <= 40.9176)) & ((new_frame_cleaned2.pickup_longitude >= -74.15) & (new_frame_cleaned2.pickup_longitude <= -73.7004)))]\n",
    "print(\"Pickups outside of NYC are removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame_cleaned2[(((new_frame_cleaned2.dropoff_latitude >= 40.5774) & (new_frame_cleaned2.dropoff_latitude <= 40.9176)) & ((new_frame_cleaned2.dropoff_longitude >= -74.15) & (new_frame_cleaned2.dropoff_longitude <= -73.7004)))]\n",
    "print(\"Dropoffs outside of NYC are removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2[\"pickup_cluster\"] = regions.predict(new_frame_cleaned2[[\"pickup_latitude\", \"pickup_longitude\"]])\n",
    "print(\"Pickup Clusters are assigned\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "jan_2016_data = pickup_10min_bins(new_frame_cleaned2, 1, 2016)\n",
    "print(\"Pickup time bins are assigned\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "jan_2016_timeBin_groupBy = jan_2016_data[[\"pickup_cluster\", \"time_bin\", \"trip_distance\"]].groupby(by = [\"pickup_cluster\", \"time_bin\"]).count()\n",
    "print(\"Pickup cluster and time bins are grouped.\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Done...\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Fraction of Total data left = \"+str(new_frame_cleaned2.shape[0]/new_frame2.shape[0]))\n",
    "print(\"Total Number of outliers removed = \"+str(new_frame2.shape[0] - new_frame_cleaned2.shape[0]))\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Total Time taken for execution of Jan 2016 data = \"+str(datetime.now() - startTime))\n",
    "print(\"-\"*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "_rQPn7YkYa04",
    "outputId": "a062274e-94d1-49fd-a74a-6bb07d1b5129"
   },
   "outputs": [],
   "source": [
    "jan_2016_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "eVKuiT1AYa06",
    "outputId": "8191c34c-2738-4199-9862-1c56ccf0dfeb"
   },
   "outputs": [],
   "source": [
    "jan_2016_timeBin_groupBy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "N5J3TEpWYa07",
    "outputId": "916e5a80-3404-4315-ad52-7b515d47ca25"
   },
   "outputs": [],
   "source": [
    "jan_2015_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "EsntCvxBYa09",
    "outputId": "c532666c-0427-414f-c8b3-abc64b0fd852"
   },
   "outputs": [],
   "source": [
    "jan_2015_timeBin_groupBy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_IStlLrYa0_"
   },
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16N7Yww0Ya1B"
   },
   "outputs": [],
   "source": [
    "# Gets the unique time bins where pickup values are present for each region.\n",
    "\n",
    "# for each cluster region we will collect all the indices of 10min intervals in which pickups happened.\n",
    "# we got an observation that there are some time bins that doesn't have any pickups.\n",
    "\n",
    "def getUniqueBinsWithPickups(dataframe):\n",
    "    values = []\n",
    "    for i in range(30):          #we have total 30 clusters\n",
    "        cluster_id = dataframe[dataframe[\"pickup_cluster\"] == i]\n",
    "        unique_clus_id = list(set(cluster_id[\"time_bin\"]))\n",
    "        unique_clus_id.sort()   #inplace sorting\n",
    "        values.append(unique_clus_id)\n",
    "    return values\n",
    "#this function is returning the indices of all the unique time_bins where there is a pickup for all the 30 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hSNMZ5P8Ya1C",
    "outputId": "17439507-964a-4af4-dd76-f8aca38dda30"
   },
   "outputs": [],
   "source": [
    "#now for Jan-2015, we have to find out, how many time_bins are there where there is no pickup in any of the cluster region\n",
    "unique_binswithPickup_Jan_2015 = getUniqueBinsWithPickups(jan_2015_data)\n",
    "for i in range(30):             #we have total 30 clusters\n",
    "    print(\"For cluster ID {}, total number of time bins with no pickup in this clutser region is {}\".format(i, (4464 - len(unique_binswithPickup_Jan_2015[i]))))\n",
    "    print(\"-\"*90)\n",
    "#there are total 4464 time bins in Jan - 2015.\n",
    "#\"unique_binswithPickup_Jan_2015\" contains all the unique time bins, where pickup happened. It contains 30 sub-arrays as there are 30 clusters\n",
    "#and each sub-array contains the unique ID of all the time bins where pickup happened in the clusters which is the index of that sub-array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xdd3nk7bYa1G"
   },
   "source": [
    "There are two ways to fill up these values:\n",
    "\n",
    "* Fill the missing value with 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Abou369JYa1G"
   },
   "outputs": [],
   "source": [
    "# Fill the missing value with 0's.\n",
    "def fillMissingWithZero(numberOfPickups, correspondingTimeBin):\n",
    "    ind = 0\n",
    "    smoothed_regions = []\n",
    "    for c in range(0, 30):\n",
    "        smoothed_bins = []\n",
    "        for t in range(4464):    #there are total 4464 time bins in both Jan-2015 & Feb-2016.\n",
    "            if t in correspondingTimeBin[c]:   #if a time bin is present in \"correspondingTimeBin\" in cluster 'c', \n",
    "            #then it means there is a pickup, in this case, we are simply adding number of pickups, else we are adding 0.\n",
    "                smoothed_bins.append(numberOfPickups[ind])\n",
    "                ind += 1\n",
    "            else:\n",
    "                smoothed_bins.append(0)\n",
    "        smoothed_regions.extend(smoothed_bins)\n",
    "    return smoothed_regions\n",
    "#above function performs the operation in this way: if in any cluster if there is no pickup in any of the 4464 time bins, then \n",
    "#it simply appends 0 in that missing time_bin else it adds the original number of pickups in that time_bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXDLVqVBYa1H"
   },
   "source": [
    "* Fill the missing values with the avg values.\n",
    "\n",
    "    * Case 1:(values missing at the start)<br>\n",
    "      Ex1: _ _ _ x =>ceil(x/4), ceil(x/4), ceil(x/4), ceil(x/4)<br>\n",
    "      Ex2: _ _ x => ceil(x/3), ceil(x/3), ceil(x/3)\n",
    "      \n",
    "    * Case 2:(values missing in middle)<br>\n",
    "      Ex1: x _ _ y => ceil((x+y)/4), ceil((x+y)/4), ceil((x+y)/4), ceil((x+y)/4)<br>\n",
    "      Ex2: x _ _ _ y => ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5)\n",
    "      \n",
    "    * Case 3:(values missing at the end)<br>\n",
    "      Ex1: x _ _ _ => ceil(x/4), ceil(x/4), ceil(x/4), ceil(x/4)<br> \n",
    "      Ex2: x _ => ceil(x/2), ceil(x/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MezYj258Ya1I"
   },
   "outputs": [],
   "source": [
    "def smoothing(numberOfPickups, correspondingTimeBin):\n",
    "    ind = 0\n",
    "    repeat = 0\n",
    "    smoothed_region = []\n",
    "    for cluster in range(0, 30):\n",
    "        smoothed_bin = []\n",
    "        for t1 in range(4464):\n",
    "            if repeat != 0:   #this will ensure that we shall not fill the pickup values again which we already filled by smoothing\n",
    "                repeat -= 1\n",
    "            else:\n",
    "                if t1 in correspondingTimeBin[cluster]:\n",
    "                    smoothed_bin.append(numberOfPickups[ind])\n",
    "                    ind += 1\n",
    "                else:\n",
    "                    if t1 == 0:           \n",
    "    #<---------------------CASE-1:Pickups missing in the beginning------------------------>\n",
    "                        for t2 in range(t1, 4464):\n",
    "                            if t2 not in correspondingTimeBin[cluster]:\n",
    "                                continue\n",
    "                            else:\n",
    "                                right_hand_limit = t2\n",
    "                                smoothed_value = (numberOfPickups[ind]*1.0)/((right_hand_limit + 1)*1.0)\n",
    "                                for i in range(right_hand_limit + 1):\n",
    "                                    smoothed_bin.append(math.ceil(smoothed_value))\n",
    "                                ind += 1\n",
    "                                repeat = right_hand_limit - t1\n",
    "                                \n",
    "                    if t1 != 0:\n",
    "                        right_hand_limit = 0\n",
    "                        for t2 in range(t1, 4464):\n",
    "                            if t2 not in correspondingTimeBin[cluster]:\n",
    "                                continue\n",
    "                            else:\n",
    "                                right_hand_limit = t2\n",
    "                                break\n",
    "                        if right_hand_limit == 0:\n",
    "    #<---------------------CASE-2: Pickups MISSING IN THE END------------------------------>\n",
    "                            smoothed_value = (numberOfPickups[ind-1]*1.0)/(((4464 - t1)+1)*1.0)\n",
    "                            del smoothed_bin[-1]\n",
    "                            for i in range((4464 - t1)+1):\n",
    "                                smoothed_bin.append(math.ceil(smoothed_value))\n",
    "                            repeat = (4464 - t1) - 1    \n",
    "    #<---------------------CASE-3: Pickups MISSING IN MIDDLE OF TWO VALUES----------------> \n",
    "                        else: \n",
    "                            smoothed_value = ((numberOfPickups[ind-1] + numberOfPickups[ind])*1.0)/(((right_hand_limit - t1)+2)*1.0)\n",
    "                            del smoothed_bin[-1]\n",
    "                            for i in range((right_hand_limit - t1)+2):\n",
    "                                smoothed_bin.append(math.ceil(smoothed_value))\n",
    "                            ind += 1\n",
    "                            repeat = right_hand_limit - t1                        \n",
    "        smoothed_region.extend(smoothed_bin)\n",
    "    return smoothed_region\n",
    "\n",
    "# when we multiply any integer with \"1.0\", then it will be converted into float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBFrs6gNYa1K"
   },
   "outputs": [],
   "source": [
    "# How above smoothing function code is working:\n",
    "# Above smoothing function code is working on the goal of filling values with the following idea: \n",
    "\n",
    "# If there are no pickups in the beginning of time_bin means if the values are missing at time bins starting from 12:00 till \n",
    "# anytime, then we will fill pickups as follows:\n",
    "\n",
    "# Case 1:(pickups missing at the start):\n",
    "# Ex1: _ _ _ x =>ceil(x/4), ceil(x/4), ceil(x/4), ceil(x/4)\n",
    "# Ex2: _ _ x => ceil(x/3), ceil(x/3), ceil(x/3)\n",
    "\n",
    "# Case 2:(pickups missing at the end)\n",
    "# Ex1: x _ _ _ => ceil(x/4), ceil(x/4), ceil(x/4), ceil(x/4)\n",
    "# Ex2: x _ => ceil(x/2), ceil(x/2)\n",
    "\n",
    "# Case 3:(values missing in middle)\n",
    "# Ex1: x _ _ y => ceil((x+y)/4), ceil((x+y)/4), ceil((x+y)/4), ceil((x+y)/4)\n",
    "# Ex2: x _ _ _ y => ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5), ceil((x+y)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1E7IlihYa1L"
   },
   "outputs": [],
   "source": [
    "jan_2015_fillZero = fillMissingWithZero(jan_2015_timeBin_groupBy[\"trip_distance\"].values, unique_binswithPickup_Jan_2015)\n",
    "# here in jan_2015_timeBin_groupBy dataframe the \"trip_distance\" represents the number of pickups that are happened.\n",
    "jan_2015_fillSmooth = smoothing(jan_2015_timeBin_groupBy[\"trip_distance\"].values, unique_binswithPickup_Jan_2015)\n",
    "\n",
    "#\"unique_binswithPickup_Jan_2015\" contains all the unique time bins, where pickup happened. It contains 30 sub-arrays as there are 30 clusters\n",
    "#and each sub-array contains the unique ID of all the time bins where pickup happened in the clusters which is the index of that sub-array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kG05oBnOYa1N"
   },
   "outputs": [],
   "source": [
    "def countZeros(num):\n",
    "    count = 0\n",
    "    for i in num:\n",
    "        if i == 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2qWTEMeYa1P"
   },
   "outputs": [],
   "source": [
    "print(\"Number of values filled with zero in zero fill data= \"+str(countZeros(jan_2015_fillZero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiidR2G8Ya1R"
   },
   "outputs": [],
   "source": [
    "print(\"Sanity check for number of zeros in smoothed data = \"+str(countZeros(jan_2015_fillSmooth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OaQ2KVcLYa1U"
   },
   "source": [
    "There are total 30 clusters. Each cluster has 4464 time bins. After smoothing or fillWithZero, each of 4464 time bin has a pickup. So, there should be a total of <b>4464*30 = 133920</b> pickup values present for the month of January 2015. Let's Check, is it correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pe1BXJDPYa1V"
   },
   "outputs": [],
   "source": [
    "print(\"Total number of pickup values = \"+str(len(jan_2015_fillZero)))\n",
    "print(\"Total number of pickup values = \"+str(len(jan_2015_fillSmooth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kwCCUNHsYa1X"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18, 10))\n",
    "plt.plot(jan_2015_fillZero[4464*25:4464*26], label = \"Filled With Zero\")\n",
    "plt.plot(jan_2015_fillSmooth[4464*25:4464*26], label = \"Filled with Avg Values(Smooth)\")\n",
    "plt.legend(bbox_to_anchor=(1, 1), fontsize = 18)\n",
    "plt.ylim(0,9)\n",
    "plt.tick_params(labelsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sZ3nqTlYa1b"
   },
   "source": [
    "#### Above plot is a plot of all the pickup values for cluster 26th. It shows that all the zero filled are separated by avg filled values. The minimum average filled value is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_bpyI1c5Ya1b"
   },
   "source": [
    "Ques: Why we choose, these methods and which method is used for which data?\n",
    "\n",
    "Ans: consider we have data of some month in 2015 jan 1st, 10 _ _ _ 20, i.e there are 10 pickups that are happened in 1st \n",
    "10st 10min interval, 0 pickups happened in 2nd 10mins interval, 0 pickups happened in 3rd 10min interval. \n",
    "and 20 pickups happened in 4th 10min interval.\n",
    "In fillMissingWithZero method we replace these values like 10, 0, 0, 0, 20.\n",
    "Whereas in \"smoothing\" method we replace these values as 6,6,6,6,6, if you can check the number of pickups that are happened \n",
    "in first 40min are same in both cases, but if you can observe that we looking at the future values. \n",
    "When you are using smoothing we are looking at the future number of pickups which might cause a data leakage.\n",
    "\n",
    "So we use smoothing for jan 2015th data since it acts as our training data\n",
    "and we use simple fill_misssing method for Jan 2016 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5ewkOzIYa1c"
   },
   "outputs": [],
   "source": [
    "unique_binswithPickup_Jan_2016 = getUniqueBinsWithPickups(jan_2016_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1J_H9DIYa1d"
   },
   "outputs": [],
   "source": [
    "# Jan-2015 data is smoothed, Jan-2016 data missing values are filled with zero\n",
    "jan_2016_fillZero = fillMissingWithZero(jan_2016_timeBin_groupBy[\"trip_distance\"].values, unique_binswithPickup_Jan_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_L5zQ8RYa1e"
   },
   "outputs": [],
   "source": [
    "regionWisePickup_Jan_2016 = []\n",
    "for i in range(30):\n",
    "    regionWisePickup_Jan_2016.append(jan_2016_fillZero[4464*i:((4464*i)+4464)])\n",
    "#\"regionWisePickup_Jan_2016\" is a list of lists which contains 30 sub lists, where the index of each sub-list is the \n",
    "#corresponding cluster number and the element of each sub-list is the pickup value. So, we know that there are 4464 time bins \n",
    "#in Jan 2016, hence, each sub-list is of size 4464."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqPmj5NOYa1f"
   },
   "outputs": [],
   "source": [
    "print(len(regionWisePickup_Jan_2016))\n",
    "print(len(regionWisePickup_Jan_2016[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xe5kmHGYa1g"
   },
   "outputs": [],
   "source": [
    "# def find_missing_timeBins(IDs):\n",
    "#     missing = []\n",
    "#     for i in range(len(unique_binswithPickup_Jan_2015[IDs]) - 1):\n",
    "#         j = unique_binswithPickup_Jan_2015[IDs][i]\n",
    "#         k = unique_binswithPickup_Jan_2015[IDs][i+1]\n",
    "#         if (k-j) > 1:\n",
    "#             for l in range(j+1, k):\n",
    "#                 missing.append(l)\n",
    "#     return missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXenuqV3Ya1h"
   },
   "source": [
    "# 4. Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdJdBQmuYa1h"
   },
   "source": [
    "Now we get into modelling in order to forecast the pickup densities for the months of Jan-2016 for which we are using multiple models with two variations. \n",
    "1. Using Ratios of the 2016 data to the 2015 data i.e $R_{t} = P^{2016}_{t} / P^{2015}_{t}$\n",
    "2. Using Previous known values of the 2016 data itself to predict the future values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOEj57PqYa1h"
   },
   "source": [
    "### Preparing dataframe with $x_i$ as Jan- 2015 pickups and $y_i$ as Jan-2016 Pickups, with ratios as $Pickup^{2016} / Pickup^{2015}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ApdNJQUvYa1i"
   },
   "outputs": [],
   "source": [
    "Ratios_DF = pd.DataFrame()\n",
    "Ratios_DF[\"Given\"] = jan_2015_fillSmooth\n",
    "Ratios_DF[\"Prediction\"] = jan_2016_fillZero\n",
    "Ratios_DF[\"Ratio\"] = Ratios_DF[\"Prediction\"]*1.0/Ratios_DF[\"Given\"]*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7PGAjihYa1j"
   },
   "outputs": [],
   "source": [
    "Ratios_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNuRxkXsYa1k"
   },
   "outputs": [],
   "source": [
    "Ratios_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9SSOGMXYa1l"
   },
   "outputs": [],
   "source": [
    "print(\"Total Number of zeros in Ratio column = \"+str(Ratios_DF[\"Ratio\"].value_counts()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IsJwdP5JYa1m"
   },
   "outputs": [],
   "source": [
    "print(\"Total Number of zeros in Prediction column = \"+str(Ratios_DF[\"Prediction\"].value_counts()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnW1I9uKYa1n"
   },
   "source": [
    "## Simple Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HXugYEXYa1o"
   },
   "source": [
    "The First Model used is the Moving Averages Model which uses the previous n values in order to predict the next value.<br> \n",
    "Using Ratio Values - $R_{t} = ( R_{t-1} + R_{t-2} + R_{t-3} .... R_{t-n} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZrPyGkh4Ya1o"
   },
   "outputs": [],
   "source": [
    "def simple_moving_average_ratios(ratios):\n",
    "    predicted_ratio = (ratios[\"Ratio\"].values)[0]\n",
    "    predicted_ratio_values = []\n",
    "    predicted_pickup_values = []\n",
    "    absolute_error = []\n",
    "    squared_error = []\n",
    "    window_size = 3\n",
    "    for i in range(4464*30):\n",
    "        if i % 4464 == 0:\n",
    "            predicted_ratio_values.append(0)\n",
    "            predicted_pickup_values.append(0)\n",
    "            absolute_error.append(0)\n",
    "            squared_error.append(0)\n",
    "        else:\n",
    "            predicted_ratio_values.append(predicted_ratio)\n",
    "            predicted_pickup_values.append(int(predicted_ratio_values[i] * ratios[\"Given\"].values[i]))\n",
    "            absolute_error.append(abs(int(predicted_ratio_values[i] * ratios[\"Given\"].values[i]) - ratios[\"Prediction\"].values[i]))\n",
    "            \n",
    "            error = math.pow((int(predicted_ratio_values[i] * ratios[\"Given\"].values[i]) - ratios[\"Prediction\"].values[i]), 2)\n",
    "            squared_error.append(error)\n",
    "            \n",
    "        if (i+1)>=window_size:\n",
    "            predicted_ratio = sum(ratios[\"Ratio\"].values[(i+1) - window_size:(i+1)])\n",
    "            predicted_ratio = predicted_ratio/window_size\n",
    "        else:\n",
    "            predicted_ratio = sum(ratios[\"Ratio\"].values[0:(i+1)])\n",
    "            predicted_ratio = predicted_ratio/(i+1)\n",
    "            \n",
    "    ratios[\"Simple_Moving_Average_Ratios_Pred\"] = predicted_pickup_values\n",
    "    ratios[\"Simple_Moving_Average_Ratios_AbsError\"] = absolute_error\n",
    "    mean_absolute_percentage_error = (sum(absolute_error)/len(absolute_error)) / (sum(ratios[\"Prediction\"]) / len(ratios[\"Prediction\"]))\n",
    "    mean_sq_error = sum(squared_error)/len(squared_error)\n",
    "    return ratios, mean_absolute_percentage_error, mean_sq_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oRMEf8xYa1p"
   },
   "outputs": [],
   "source": [
    "# here, if we calculate absolute percentage error by this formulae: \n",
    "# \"error = (abs(int(predicted_ratio_values[i] * ratios[\"Given\"].values[i]) - ratios[\"Prediction\"].values[i])) / ratios[\"Prediction\"].values[i]\"\n",
    "#then it will lead to divide by zero problem because many of the values in \" ratios[\"Prediction\"].values[i]\" are zeros.\n",
    "# so we used this method to calculate mean absolute percentage error: \"mean of error/mean of real values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r71qpuXbYa1q"
   },
   "source": [
    "For the above the Hyperparameter is the window-size (n) which is tuned manually and it is found that the window-size of 3 is optimal for getting the best results using Moving Averages using previous Ratio values therefore we get $R_{t} = ( R_{t-1} + R_{t-2} + R_{t-3}) / 3$. We can easily plug any 3 to 5 values which seems to be most effective in reducing MAPE. Like here, we can use 1,2,3,4,5 as the values for window_size and calculate optimal window_size value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfQK8kmoYa1s"
   },
   "source": [
    "Next we use the Moving averages of the 2016  values itself to predict the future value using $P_{t} = ( P_{t-1} + P_{t-2} + P_{t-3} .... P_{t-n} )/n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrKRmJO0Ya1u"
   },
   "outputs": [],
   "source": [
    "def simple_moving_average_predictions(ratios):\n",
    "    predicted_pickup = (ratios[\"Prediction\"].values)[0]\n",
    "    predicted_pickup_values = []\n",
    "    absolute_error = []\n",
    "    squared_error = []\n",
    "    window_size = 2\n",
    "    for i in range(4464*30):\n",
    "        if i % 4464 == 0:\n",
    "            predicted_pickup_values.append(0)\n",
    "            absolute_error.append(0)\n",
    "            squared_error.append(0)\n",
    "        else:\n",
    "            predicted_pickup_values.append(predicted_pickup)\n",
    "            absolute_error.append(abs(predicted_pickup_values[i] - ratios[\"Prediction\"].values[i]))\n",
    "            \n",
    "            error = math.pow((predicted_pickup_values[i] - ratios[\"Prediction\"].values[i]), 2)\n",
    "            squared_error.append(error)\n",
    "            \n",
    "        if (i+1)>=window_size:\n",
    "            predicted_pickup = sum(ratios[\"Prediction\"].values[(i+1) - window_size:(i+1)])\n",
    "            predicted_pickup = predicted_pickup/window_size\n",
    "        else:\n",
    "            predicted_pickup = sum(ratios[\"Prediction\"].values[0:(i+1)])\n",
    "            predicted_pickup = predicted_pickup/(i+1)\n",
    "            \n",
    "    ratios[\"Simple_Moving_Average_Predictions_Pred\"] = predicted_pickup_values\n",
    "    ratios[\"Simple_Moving_Average_Predictions_AbsError\"] = absolute_error\n",
    "    mean_absolute_percentage_error = (sum(absolute_error)/len(absolute_error)) / (sum(ratios[\"Prediction\"]) / len(ratios[\"Prediction\"]))\n",
    "    mean_sq_error = sum(squared_error)/len(squared_error)\n",
    "    return ratios, mean_absolute_percentage_error, mean_sq_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GHt3k62IYa1v"
   },
   "source": [
    "For the above the Hyperparameter is the window-size (n) which is tuned manually and it is found that the window-size of 2 is optimal for getting the best results using Moving Averages using previous 2016 values therefore we get $P_{t} = ( P_{t-1} + P_{t-2} ) / 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrIIZMQUYa1v"
   },
   "source": [
    "## Simple Weighted Moving Average\n",
    "The Moving Avergaes Model used gave equal importance to all the values in the window used, but we know intuitively that the future is more likely to be similar to the latest values and less similar to the older values. Weighted Averages converts this analogy into a mathematical relationship giving the highest weight while computing the averages to the latest previous value and decreasing weights to the subsequent older ones<br>\n",
    "\n",
    "Weighted Moving Averages using Ratio Values: $R_t = (N*R_{t-1} + N-1*R_{t-2} + N-2*R_{T-3} + ...) / N*(N+1)/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xhaQCXcYa1v"
   },
   "outputs": [],
   "source": [
    "def weighted_moving_average_ratios(ratios):\n",
    "    predicted_ratio = (ratios[\"Ratio\"].values)[0]\n",
    "    predicted_ratio_values = []\n",
    "    predicted_pickup_values = []\n",
    "    absolute_error = []\n",
    "    squared_error = []\n",
    "    window_size = 4\n",
    "    for i in range(4464*30):\n",
    "        if i % 4464 == 0:\n",
    "            predicted_ratio_values.append(0)\n",
    "            predicted_pickup_values.append(0)\n",
    "            absolute_error.append(0)\n",
    "            squared_error.append(0)\n",
    "        else:\n",
    "            predicted_ratio_values.append(predicted_ratio)\n",
    "            predicted_pickup_values.append(int(predicted_ratio_values[i] * ratios[\"Given\"].values[i]))\n",
    "            absolute_error.append(abs(int(predicted_ratio_values[i] * ratios[\"Given\"].values[i]) - ratios[\"Prediction\"].values[i]))\n",
    "            \n",
    "            error = math.pow((int(predicted_ratio_values[i] * ratios[\"Given\"].values[i]) - ratios[\"Prediction\"].values[i]), 2)\n",
    "            squared_error.append(error)\n",
    "            \n",
    "        if (i+1)>=window_size:\n",
    "            sumOfRatios = 0\n",
    "            sumOfWeights = 0\n",
    "            for j in range(window_size, 0, -1):\n",
    "                sumOfRatios = sumOfRatios + j*(ratios[\"Ratio\"].values)[i -window_size + j]\n",
    "                sumOfWeights = sumOfWeights + j\n",
    "            predicted_ratio = sumOfRatios/sumOfWeights\n",
    "        else:\n",
    "            sumOfRatios = 0\n",
    "            sumOfWeights = 0\n",
    "            for j in range(i+1, 0, -1):\n",
    "                sumOfRatios = sumOfRatios + j*(ratios[\"Ratio\"].values)[j-1]\n",
    "                sumOfWeights = sumOfWeights + j\n",
    "            predicted_ratio = sumOfRatios/sumOfWeights\n",
    "    \n",
    "    ratios[\"Weighted_Moving_Average_Ratios_Pred\"] = predicted_pickup_values\n",
    "    ratios[\"Weighted_Moving_Average_Ratios_AbsError\"] = absolute_error\n",
    "    mean_absolute_percentage_error = (sum(absolute_error)/len(absolute_error)) / (sum(ratios[\"Prediction\"]) / len(ratios[\"Prediction\"]))\n",
    "    mean_sq_error = sum(squared_error)/len(squared_error)\n",
    "    return ratios, mean_absolute_percentage_error, mean_sq_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeaCfY4TYa1x"
   },
   "source": [
    "For the above the Hyperparameter is the window-size (n) which is tuned manually and it is found that the window-size of 4 is optimal for getting the best results using Weighted Moving Averages using previous Ratio values therefore we get $ R_{t} = ( 4*R_{t-1} + 3*R_{t-2} + 2*R_{t-3} + 1*R_{t-4})/10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6Juac-4Ya1x"
   },
   "source": [
    "Weighted Moving Averages using Previous 2016 Values - $P_{t} = ( N*P_{t-1} + (N-1)*P_{t-2} + (N-2)*P_{t-3} .... 1*P_{t-n} )/(N*(N+1)/2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wr1FwlAmYa1y"
   },
   "outputs": [],
   "source": [
    "def weighted_moving_average_predictions(ratios):\n",
    "    predicted_pickup = (ratios[\"Prediction\"].values)[0]\n",
    "    predicted_pickup_values = []\n",
    "    absolute_error = []\n",
    "    squared_error = []\n",
    "    window_size = 2\n",
    "    for i in range(4464*30):\n",
    "        if i % 4464 == 0:\n",
    "            predicted_pickup_values.append(0)\n",
    "            absolute_error.append(0)\n",
    "            squared_error.append(0)\n",
    "        else:\n",
    "            predicted_pickup_values.append(predicted_pickup)\n",
    "            absolute_error.append(abs(predicted_pickup_values[i] - ratios[\"Prediction\"].values[i]))\n",
    "            \n",
    "            error = math.pow(int(predicted_pickup_values[i] - ratios[\"Prediction\"].values[i]), 2)\n",
    "            squared_error.append(error)\n",
    "            \n",
    "        if (i+1)>=window_size:\n",
    "            sumPickups = 0\n",
    "            sumOfWeights = 0\n",
    "            for j in range(window_size, 0, -1):\n",
    "                sumPickups = sumPickups + j*(ratios[\"Prediction\"].values)[i -window_size + j]\n",
    "                sumOfWeights = sumOfWeights + j\n",
    "            predicted_pickup = sumPickups/sumOfWeights\n",
    "        else:\n",
    "            sumPickups = 0\n",
    "            sumOfWeights = 0\n",
    "            for j in range(i+1, 0, -1):\n",
    "                sumPickups += j*(ratios[\"Prediction\"].values)[j-1]\n",
    "                sumOfWeights += j\n",
    "            predicted_pickup = sumPickups/sumOfWeights\n",
    "    \n",
    "    ratios[\"Weighted_Moving_Average_Predictions_Pred\"] = predicted_pickup_values\n",
    "    ratios[\"Weighted_Moving_Average_Predictions_AbsError\"] = absolute_error\n",
    "    mean_absolute_percentage_error = (sum(absolute_error)/len(absolute_error)) / (sum(ratios[\"Prediction\"]) / len(ratios[\"Prediction\"]))\n",
    "    mean_sq_error = sum(squared_error)/len(squared_error)\n",
    "    return ratios, mean_absolute_percentage_error, mean_sq_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l8Sa8qIAYa10"
   },
   "source": [
    "For the above the Hyperparameter is the window-size (n) which is tuned manually and it is found that the window-size of 2 is optimal for getting the best results using Weighted Moving Averages using previous Ratio values therefore we get $ P_t = ( 2*P_{t-1} + 1*P_{t-2})/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m10MaQdWYa11"
   },
   "source": [
    "## Exponential Weighted Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bC8jQw3GYa11"
   },
   "source": [
    "Here we will use previous predicted result along with previous actual value to make predictions for next value.\n",
    "$R^{'}_{t} = \\alpha*R_{t-1} + (1-\\alpha)*R^{'}_{t-1}$\n",
    "\n",
    "$R^′_t$ is the current predicted ratio.<br>\n",
    "$R^′_{t-1}$ is the previous predicted ratio.<br>\n",
    "$R_{t-1}$ is the actual previous ratio.<br>\n",
    "Let say alpha = 0.7.<br>\n",
    "Now, when alpha = 0.7, then it means we are giving 70% weightage to the previous predicted ratio and 30% weightage to the previous actual ratio.<br>\n",
    "$R^′_0$ = 0<br>\n",
    "$R^′_1 = 0.7*R^′_0 + 0.3*R_0$<br>\n",
    "$R^′_2 = 0.7*R^′_1 + 0.3*R_1$<br>\n",
    "$R^′_3 = 0.7*R^′_2 + 0.3*R_2$<br>\n",
    "Let’s take $R^′_3$.<br>\n",
    "$R^′_3  = 0.3* R_2 + 0.7* R^′_2$<br>\n",
    "$R^′_3  = 0.3* R_2 + 0.7*(0.3* R_1 + 0.7*R^′_1)$<br>\n",
    "$R^′_3  = 0.3* R_2 + 0.7 * ( 0.3* R_1 + 0.7* ( 0.3* R_0 + 0.7* R^′_0 ) )$<br>\n",
    "$R^′_3  = 0.3* R_3 + 0.7*0.3* R_1 + 0.7*0.7*0.3* R_0 + 0.7*0.7*0.7* R^′_0$<br>\n",
    "$R^′_3  = 0.3* R_3 + 0.7*0.3* R_1 + 0.7*0.7*0.3* R_0 + 0$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIAp6LtgYa12"
   },
   "outputs": [],
   "source": [
    "def exponential_weighted_moving_average_ratios(ratios):\n",
    "    predicted_ratio = (ratios[\"Ratio\"].values)[0]\n",
    "    predicted_ratio_values = []\n",
    "    predicted_pickup_values = []\n",
    "    absolute_error = []\n",
    "    squared_error = []\n",
    "    alpha = 0.5\n",
    "    for i in range(4464*30):\n",
    "        if i % 4464 == 0:\n",
    "            predicted_ratio_values.append(0)\n",
    "            predicted_pickup_values.append(0)\n",
    "            absolute_error.append(0)\n",
    "            squared_error.append(0)\n",
    "        else:\n",
    "            predicted_ratio_values.append(predicted_ratio)\n",
    "            predicted_pickup_values.append(int(predicted_ratio_values[i] * ratios[\"Given\"].values[i]))\n",
    "            absolute_error.append(abs(int(predicted_ratio_values[i] * ratios[\"Given\"].values[i]) - ratios[\"Prediction\"].values[i]))\n",
    "            error = math.pow((int(predicted_ratio_values[i] * ratios[\"Given\"].values[i]) - ratios[\"Prediction\"].values[i]), 2)\n",
    "            squared_error.append(error)\n",
    "            predicted_ratio = alpha*predicted_ratio + ((1-alpha)*ratios[\"Ratio\"].values[i-1])\n",
    "            \n",
    "            \n",
    "    ratios[\"Exponential_Weighted_Moving_Average_Ratios_Pred\"] = predicted_pickup_values\n",
    "    ratios[\"Exponential_Weighted_Moving_Average_Ratios_AbsError\"] = absolute_error\n",
    "    mean_absolute_percentage_error = (sum(absolute_error)/len(absolute_error)) / (sum(ratios[\"Prediction\"]) / len(ratios[\"Prediction\"]))\n",
    "    mean_sq_error = sum(squared_error)/len(squared_error)\n",
    "    return ratios, mean_absolute_percentage_error, mean_sq_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufDdnaK3Ya13"
   },
   "source": [
    "Here, above, alpha is a hyper-parameter which needs to be tuned manually. It is found that alpha = 0.5 gives lowest MAPE value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIXXJi4yYa13"
   },
   "source": [
    "$P^{'}_{t} = \\alpha*P_{t-1} + (1-\\alpha)*P^{'}_{t-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKlBbB7nYa14"
   },
   "outputs": [],
   "source": [
    "def exponential_weighted_moving_average_predictions(ratios):\n",
    "    predicted_pickup = (ratios[\"Prediction\"].values)[0]\n",
    "    predicted_pickup_values = []\n",
    "    absolute_error = []\n",
    "    squared_error = []\n",
    "    alpha = 0.5\n",
    "    for i in range(4464*30):\n",
    "        if i % 4464 == 0:\n",
    "            predicted_pickup_values.append(0)\n",
    "            absolute_error.append(0)\n",
    "            squared_error.append(0)\n",
    "        else:\n",
    "            predicted_pickup_values.append(predicted_pickup)\n",
    "            absolute_error.append(abs(predicted_pickup_values[i] - ratios[\"Prediction\"].values[i]))\n",
    "            error = math.pow((predicted_pickup_values[i] - ratios[\"Prediction\"].values[i]), 2)\n",
    "            squared_error.append(error)\n",
    "            predicted_pickup = alpha*predicted_pickup + ((1-alpha)*ratios[\"Prediction\"].values[i-1])\n",
    "            \n",
    "            \n",
    "    ratios[\"Exponential_Weighted_Moving_Average_Predictions_Pred\"] = predicted_pickup_values\n",
    "    ratios[\"Exponential_Weighted_Moving_Average_Predictions_AbsError\"] = absolute_error\n",
    "    mean_absolute_percentage_error = (sum(absolute_error)/len(absolute_error)) / (sum(ratios[\"Prediction\"]) / len(ratios[\"Prediction\"]))\n",
    "    mean_sq_error = sum(squared_error)/len(squared_error)\n",
    "    return ratios, mean_absolute_percentage_error, mean_sq_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4id0ODeYa15"
   },
   "outputs": [],
   "source": [
    "# r1, mape1, mse1 = simple_moving_average_ratios(Ratios_DF)\n",
    "# r2, mape2, mse2 = simple_moving_average_predictions(Ratios_DF)\n",
    "# r3, mape3, mse3 = weighted_moving_average_ratios(Ratios_DF)\n",
    "r4, mape4, mse4 = weighted_moving_average_predictions(Ratios_DF)\n",
    "# r5, mape5, mse5 = exponential_weighted_moving_average_ratios(Ratios_DF)\n",
    "# r6, mape6, mse6 = exponential_weighted_moving_average_predictions(Ratios_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k98fYZCuYa16"
   },
   "outputs": [],
   "source": [
    "error_table_baseline = pd.DataFrame(columns = [\"Model\", \"MAPE(%)\", \"MSE\"])\n",
    "\n",
    "# error_table_baseline = error_table_baseline.append(pd.DataFrame([[\"Simple Moving Average Ratios\", mape1*100, mse1]], columns = [\"Model\", \"MAPE(%)\", \"MSE\"]))\n",
    "# error_table_baseline = error_table_baseline.append(pd.DataFrame([[\"Simple Moving Average Predictions\", mape2*100, mse2]], columns = [\"Model\", \"MAPE(%)\", \"MSE\"]))\n",
    "# error_table_baseline = error_table_baseline.append(pd.DataFrame([[\"Weighted Moving Average Ratios\", mape3*100, mse3]], columns = [\"Model\", \"MAPE(%)\", \"MSE\"]))\n",
    "error_table_baseline = error_table_baseline.append(pd.DataFrame([[\"Weighted Moving Average Predictions\", mape4*100, mse4]], columns = [\"Model\", \"MAPE(%)\", \"MSE\"]))\n",
    "# error_table_baseline = error_table_baseline.append(pd.DataFrame([[\"Exponential Weighted Moving Average Ratios\", mape5*100, mse5]], columns = [\"Model\", \"MAPE(%)\", \"MSE\"]))\n",
    "# error_table_baseline = error_table_baseline.append(pd.DataFrame([[\"Exponential Weighted Moving Average Predictions\", mape6*100, mse6]], columns = [\"Model\", \"MAPE(%)\", \"MSE\"]))\n",
    "\n",
    "error_table_baseline.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9-oL8jLYa17"
   },
   "outputs": [],
   "source": [
    "error_table_baseline.style.highlight_min(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-B43GSKtYa18"
   },
   "source": [
    "<b>Plese Note:-</b> The above comparisons are made using Jan 2015 and Jan 2016 only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dw9-bJ2PYa18"
   },
   "source": [
    "<b>From the above error table it is inferred that the best forecasting model for our prediction would be:-</b>\n",
    "$P_{t} = ( N*P_{t-1} + (N-1)*P_{t-2} + (N-2)*P_{t-3} .... 1*P_{t-n} )/(N*(N+1)/2)$ i.e Weighted Moving Averages Predictions using 2016 Values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlLF9fCiYa18"
   },
   "source": [
    "# 5. Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-ryPltqYa19"
   },
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttCAs1tQYa19"
   },
   "source": [
    "Preparing data to be split into train and test, The below code prepares data in cumulative form which will be later split into\n",
    "test and train\n",
    "\n",
    "There are total 30 clusters and for the month of January-2016 and there are total 4464 time bins. \n",
    "For each cluster region there are 4464 time bins and so, for 30 clusters there will be 4464*30 pickup values because after \n",
    "smoothing each time bin has pickup.\n",
    "We will have a total of 4464*30 = 133920 pickup values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hs-6BHAjYa19"
   },
   "outputs": [],
   "source": [
    "#\"regionWisePickup_Jan_2016\" is a list of lists which contains 30 sub lists, where the index of each sub-list is the \n",
    "#corresponding cluster number and the element of each sub-list is the pickup value. So, we know that there are 4464 time bins \n",
    "#in Jan 2016, hence, each sub-list is of size 4464.\n",
    "#\"regionWisePickup_Jan_2016\" is a list of lists [[x1,x2,x3..4464], [x1,x2,x3..4464], [x1,x2,x3..4464], [x1,x2,x3..4464], [x1,x2,x3..4464], .. 30 lists]\n",
    "#Here, x1,x2,x3... are pickup values at time stamp 1,2,3... respectively\n",
    "\n",
    "# print(len(regionWisePickup_Jan_2016))\n",
    "# 30\n",
    "# print(len(regionWisePickup_Jan_2016[0]))\n",
    "# 4464\n",
    "\n",
    "# we take number of pickups that are happened in last 5 10min intravels\n",
    "number_of_time_stamps = 5\n",
    "\n",
    "# TruePickups varaible\n",
    "# it is list of lists\n",
    "# It will be used as true labels/ground truth. Now since we are taking previous 5 pickups as a training data for predicting\n",
    "# next pickup(here next pickup will be a true/ground truth pickup), so \"TruePickups\" will not contain first five pickups of each \n",
    "# cluster. It will contain number of pickups 4459 for each cluster. \n",
    "TruePickups = []\n",
    "\n",
    "\n",
    "# lat will contain 4464-5=4459 times latitude of cluster center for every cluster.\n",
    "# Ex: [[cent_lat 4459 times],[cent_lat 4459 times], [cent_lat 4459 times].... 30 lists]\n",
    "# it is list of lists\n",
    "lat = []\n",
    "\n",
    "\n",
    "# lon will contain 4464-5=4459 times longitude of cluster center for every cluster.\n",
    "# Ex: [[cent_lat 4459 times],[cent_lat 4459 times], [cent_lat 4459 times].... 30 lists]\n",
    "# it is list of lists\n",
    "lon = []\n",
    "\n",
    "# we will code each day \n",
    "# sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5,sat=6\n",
    "day_of_week = []\n",
    "\n",
    "\n",
    "# for every cluster we will be adding 4459 values, each value represent to which day of the week that pickup bin belongs to\n",
    "# it is list of lists\n",
    "\n",
    "# feat is a numbpy array, of shape (133770, 5). {4459*30 = 133770.}\n",
    "# each row corresponds to an entry in our data\n",
    "# for the first row we will have [f0,f1,f2,f3,f4] fi=number of pickups happened in i+1st 10min intravel(bin)\n",
    "# the second row will have [f1,f2,f3,f4,f5]\n",
    "# the third row will have [f2,f3,f4,f5,f6]\n",
    "# and so on...\n",
    "feat = []\n",
    "\n",
    "\n",
    "centerOfRegions = regions.cluster_centers_\n",
    "feat = [0]*number_of_time_stamps\n",
    "for i in range(30):\n",
    "    lat.append([centerOfRegions[i][0]]*4459) \n",
    "    lon.append([centerOfRegions[i][1]]*4459)\n",
    "    #1 January 2016 is a Friday so we start our day from 5: \"(int(j/144))%7+5\"\n",
    "    # Our prediction starts from 5th 10min interval since we need to have number of pickups that are happened in last 5 pickup bins.\n",
    "    day_of_week.append([int(((int(j/144)%7)+5)%7) for j in range(5, 4464)])\n",
    "    #\"regionWisePickup_Jan_2016\" is a list of lists which contains 30 sub lists, where the index of each sub-list is the \n",
    "    #corresponding cluster number and the element of each sub-list is the pickup value. So, we know that there are 4464 time bins \n",
    "    #in Jan 2016, hence, each sub-list is of size 4464.\n",
    "    #\"regionWisePickup_Jan_2016\" is a list of lists [[x1,x2,x3..4464], [x1,x2,x3..4464], [x1,x2,x3..4464], [x1,x2,x3..4464], [x1,x2,x3..4464], .. 30 lists]\n",
    "    #Here, x1,x2,x3... are pickup values at time stamp 1,2,3... respectively\n",
    "    feat = np.vstack((feat, [regionWisePickup_Jan_2016[i][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_Jan_2016[i]) - (number_of_time_stamps))]))\n",
    "    TruePickups.append(regionWisePickup_Jan_2016[i][5:])\n",
    "    #output contains pickup values of all the regions and of each time stamp, except first 5 time stamp pickups of each region.\n",
    "feat = feat[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDUNlVRgYa1-"
   },
   "outputs": [],
   "source": [
    "len(lat[0])*len(lat) == len(lon[0])*len(lon) == len(day_of_week[0])*len(day_of_week) == 4459*30 == len(feat) == len(TruePickups[0])*len(TruePickups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wOxZcXhsYa1_"
   },
   "outputs": [],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgcjmewbYa2A"
   },
   "source": [
    "### Adding Predictions of Weighted Moving Average Predictions as a feature in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQvaUxe1Ya2A"
   },
   "source": [
    "Getting the predictions of weighted moving averages to be used as a feature in cumulative form.\n",
    "\n",
    "Upto now we computed 8 features for every data point that starts from 50th min of the day.\n",
    "1. cluster center latitude\n",
    "2. cluster center longitude\n",
    "3. day of the week \n",
    "4. f_t_1: number of pickups that are happened previous t-1st 10min interval\n",
    "5. f_t_2: number of pickups that are happened previous t-2nd 10min interval\n",
    "6. f_t_3: number of pickups that are happened previous t-3rd 10min interval\n",
    "7. f_t_4: number of pickups that are happened previous t-4th 10min interval\n",
    "8. f_t_5: number of pickups that are happened previous t-5th 10min interval\n",
    "\n",
    "From the baseline models we said that the weighted moving avarage predictions gives us the best error.\n",
    "We will try to add the same weighted moving avarage predictions at time t as a feature to our data.<br>\n",
    "Weighted Moving Average -> $P_{t} = ( N*P_{t-1} + (N-1)*P_{t-2} + (N-2)*P_{t-3} .... 1*P_{t-n} )/(N*(N+1)/2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCjYS9urYa2B"
   },
   "outputs": [],
   "source": [
    "# \"predicted_pickup_values\": it is a temporary array that store weighted moving avarag prediction values for each 10min intervl, \n",
    "# for each cluster it will get reset.\n",
    "# for every cluster it contains 4464 values\n",
    "predicted_pickup_values = []\n",
    "\n",
    "# \"predicted_pickup_values_list\"\n",
    "# it is list of lists\n",
    "# predict_list is a list of lists [[x5,x6,x7..x4463], [x5,x6,x7..x4463], [x5,x6,x7..x4463], ... 40 lists]\n",
    "predicted_pickup_values_list = []\n",
    "\n",
    "predicted_value = -1  #it will contain cuurent predicted_value. Default is given -1 which will be replaced later\n",
    "\n",
    "window_size = 2\n",
    "for i in range(30):\n",
    "    for j in range(4464):\n",
    "        if j == 0:\n",
    "            predicted_value = regionWisePickup_Jan_2016[i][j]\n",
    "            predicted_pickup_values.append(0)\n",
    "        else:\n",
    "            if j>=window_size:\n",
    "                sumPickups = 0\n",
    "                sumOfWeights = 0\n",
    "                for k in range(window_size, 0, -1):\n",
    "                    sumPickups += k*(regionWisePickup_Jan_2016[i][j -window_size + (k - 1)])\n",
    "                    sumOfWeights += k\n",
    "                predicted_value = int(sumPickups/sumOfWeights)\n",
    "                predicted_pickup_values.append(predicted_value)\n",
    "            else:\n",
    "                sumPickups = 0\n",
    "                sumOfWeights = 0\n",
    "                for k in range(j, 0, -1):\n",
    "                    sumPickups += k*regionWisePickup_Jan_2016[i][k-1]\n",
    "                    sumOfWeights += k\n",
    "                predicted_value = int(sumPickups/sumOfWeights)\n",
    "                predicted_pickup_values.append(predicted_value)\n",
    "                \n",
    "    predicted_pickup_values_list.append(predicted_pickup_values[5:])\n",
    "    predicted_pickup_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJCF4mPPYa2B"
   },
   "outputs": [],
   "source": [
    "len(predicted_pickup_values_list[0])*len(predicted_pickup_values_list) == 4459*30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZZ9LHRrYa2D"
   },
   "source": [
    "### Adding Top 5 Frequencies and Amplitudes of Fourier Transform as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRgxmJZ9Ya2D"
   },
   "source": [
    "Fourier Transform says that whenever we have a repeating pattern in a wave like here we have a repeating pattern of pickups in 24hrs of time period, this repeating wave can be decomposed into sum of multiple sine waves. Each sine wave will have some frequency and amplitude. Now we can represent our original wave from time-domain to frequency-domain, where frequency will be presented on x-axis and amplitude will presented on y-axis. In frequency domain the x-axis frequencies will be discrete frequencies of individual sine waves and y-axis amplitudes will be their corresponding amplitude values.<br><br>\n",
    "\n",
    "In time-series data whenever we have a repeating pattern then the Fourier decomposed frequencies and their amplitude can be added as a features in our data. As these features are very useful particularly when we have a repeating pattern in a time-series data.<br><br>\n",
    "\n",
    "Intuitively, amplitude measures how significant one frequency is as compared to other frequency in a relative sense. More the amplitude, more of the time-series that sine-wave can represent. So, we can simply find the frequencies with maximal amplitudes by simple sorting them to obtain the important frequencies and their corresponding amplitudes. Finally we can add them as a feature in our data.<br><br>\n",
    "\n",
    "Now we have a total of 30 clusters in our data. In each individual cluster the pickup pattern(which is repeating) will remain same. Note that pickup pattern in different clusters will be different. So, when we plot FFT for one cluster, and let say that one cluster contains ‘n’ points, and if we take first 3 frequencies and their corresponding amplitude values as a feature, then for all of those ‘n’ points, these 3 frequencies and their corresponding amplitudes will remain same. It will change only for the points which are in another cluster.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "64WmxZb2Ya2E",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# for i in range(30):\n",
    "#     fig = plt.figure(figsize = (8, 6))\n",
    "#     plt.plot(regionWisePickup_Jan_2016[i][:4464])\n",
    "#     plt.title(\"Pickup Pattern for Cluster Region \"+str(i+1)+\", for Jan-2016.\")\n",
    "#     plt.xlabel(\"10 Minute Time Bins\")\n",
    "#     plt.ylabel(\"Number of Pickups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRjEKACOYa2G"
   },
   "source": [
    "<b>From the above graphs of pickups of each cluster we can observe that there is a repeating pattern in pickups in 24hrs time period. Now we can decompose these repeating waves by using fourier transform and use their frequencies and their corresponding amplitudes as features in our data.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQuCdBJvYa2H",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in range(30):\n",
    "#     Y  = np.abs(np.fft.fft(regionWisePickup_Jan_2016[i][0:4096]))\n",
    "#     # read more about the fftfreq: https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftfreq.html  \n",
    "#     freq = np.abs(np.fft.fftfreq(4096, 1))\n",
    "#     n = len(freq)\n",
    "#     plt.figure(figsize = (8, 6))\n",
    "#     plt.plot(freq[:], Y[:])\n",
    "#     plt.xlabel(\"Frequency\")\n",
    "#     plt.ylabel(\"Amplitude\")\n",
    "#     plt.title(\"Fourier Transformed Frequency and Amplitudes of Cluster Region \"+str(i+1)+\", for Jan 2016.\")\n",
    "#     plt.show()\n",
    "# #Here, we are concern only about positive frequencies and ampllitudes. \"np.fft.fftfreq(n, 1)\" returns the frequency of sine \n",
    "# #in a symmetric form, means there will be equal number of positive and negative sine wave frequencies. But we are concern only\n",
    "# #about positive ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xqRkXC2yYa2J"
   },
   "source": [
    "Here, there is one peak at a frequency of 1/144 which is equivalent to 24 hrs. This peak is capturing full 24hrs pickups and since during hush hours the pickups are high so therefore peak of wave is also high at 1/144. Another peak is at frequency of 1/72 which is equivalent to 12 hrs. Now this peak is capturing information of the hush hours, so therefore this peak is the highest beside DC componet which we are noth considering, and so on. If we keep on going than frequency of 1/36 is equivalent to 6 hrs, similarly frequency of 1/18 is equivalent to 3 hrs and so on.<br><br>\n",
    "\n",
    "Frequency means how many oscillations does wave completes in 1 Sec.<br><br>\n",
    "\n",
    "Now if we see in the above plot, the first peak is a DC component, which captures the amplitude of wave which comes before the part of the wave shown in the above plot. Since, the wave which we have Fourier transformed is a repeating wave, so therefore, DC componets is capturing the information of the previous part of the wave, hence, we will not consider it's amplitude and frequency. We will start taking frequency and amplitudes from the second peak onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-7ZL1NFYa2K"
   },
   "outputs": [],
   "source": [
    "amplitude_lists = []\n",
    "frequency_lists = []\n",
    "for i in range(30):\n",
    "    ampli  = np.abs(np.fft.fft(regionWisePickup_Jan_2016[i][0:4096]))\n",
    "    freq = np.abs(np.fft.fftfreq(4096, 1))\n",
    "    ampli_indices = np.argsort(-ampli)[1:]        #it will return an array of indices for which corresponding amplitude values are sorted in reverse order.\n",
    "    amplitude_values = []\n",
    "    frequency_values = []\n",
    "    for j in range(0, 9, 2):   #taking top five amplitudes and frequencies\n",
    "        amplitude_values.append(ampli[ampli_indices[j]])\n",
    "        frequency_values.append(freq[ampli_indices[j]])\n",
    "    for k in range(4459):    #those top 5 frequencies and amplitudes are same for all the points in one cluster\n",
    "        amplitude_lists.append(amplitude_values)\n",
    "        frequency_lists.append(frequency_values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ri-KPIeIYa2L"
   },
   "source": [
    "<b>Now we have built our all the features. We have finally now following 19 features in our data:</b>\n",
    "1. <b>f_t_1:</b> Number of pickups that are happened previous t-1st 10min interval\n",
    "2. <b>f_t_2:</b> Number of pickups that are happened previous t-2nd 10min interval\n",
    "3. <b>f_t_3:</b> Number of pickups that are happened previous t-3rd 10min interval\n",
    "4. <b>f_t_4:</b> Number of pickups that are happened previous t-4th 10min interval\n",
    "5. <b>f_t_5:</b> Number of pickups that are happened previous t-5th 10min interval \n",
    "6. <b>Freq1:</b> Fourier Frequency corresponding to 1st highest amplitude\n",
    "7. <b>Freq2:</b> Fourier Frequency corresponding to 2nd highest amplitude\n",
    "8. <b>Freq3:</b> Fourier Frequency corresponding to 3rd highest amplitude\n",
    "9. <b>Freq4:</b> Fourier Frequency corresponding to 4th highest amplitude\n",
    "10. <b>Freq5:</b> Fourier Frequency corresponding to 5th highest amplitude\n",
    "11. <b>Amp1:</b>  Amplitude corresponding to 1st highest fourier transformed wave.\n",
    "12. <b>Amp2:</b>  Amplitude corresponding to 2nd highest fourier transformed wave.\n",
    "13. <b>Amp3:</b>  Amplitude corresponding to 3rd highest fourier transformed wave.\n",
    "14. <b>Amp4:</b>  Amplitude corresponding to 4th highest fourier transformed wave.\n",
    "15. <b>Amp5:</b>  Amplitude corresponding to 5th highest fourier transformed wave.\n",
    "16. <b>Latitude:</b> Latitude of Cluster center.\n",
    "17. <b>Longitude:</b> Longitude of Cluster Center.\n",
    "18. <b>WeekDay:</b> Day of week of pickup.\n",
    "19. <b>WeightedAvg:</b>: Weighted Moving Average Prediction values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "unJ_-uRTYa2L"
   },
   "source": [
    "## Data Preparation for regression models\n",
    "Before we start predictions using the tree based regression models we take Jan 2016 pickup data and split it such that for every region we have 80% data in train and 20% in test, ordered date-wise for every region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbonS5gDYa2M"
   },
   "outputs": [],
   "source": [
    "print(\"size of total train data :\" +str(int(133770*0.8)))\n",
    "print(\"size of total test data :\" +str(int(133770*0.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szdA1SdyYa2P"
   },
   "outputs": [],
   "source": [
    "print(\"size of train data for one cluster:\" +str(int(4459*0.8)))\n",
    "print(\"size of total test data for one cluster:\" +str(int(4459*0.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCPGW8DeYa2S"
   },
   "outputs": [],
   "source": [
    "train_previousFive_pickups  = [feat[i*4459:(4459*i+3567)] for i in range(30)]\n",
    "test_previousFive_pickups  = [feat[(i*4459)+3567:(4459*(i+1))] for i in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UA4PEV29Ya2T"
   },
   "outputs": [],
   "source": [
    "train_fourier_frequencies = [frequency_lists[i*4459:(4459*i+3567)] for i in range(30)]\n",
    "test_fourier_frequencies = [frequency_lists[(i*4459)+3567:(4459*(i+1))] for i in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQhn-NMUYa2U"
   },
   "outputs": [],
   "source": [
    "train_fourier_amplitudes = [amplitude_lists[i*4459:(4459*i+3567)] for i in range(30)]\n",
    "test_fourier_amplitudes = [amplitude_lists[(i*4459)+3567:(4459*(i+1))] for i in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oY-lhfXrYa2W"
   },
   "outputs": [],
   "source": [
    "print(\"Train Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of training points = {}\".format(len(train_previousFive_pickups), len(train_previousFive_pickups[0]), len(train_previousFive_pickups)*len(train_previousFive_pickups[0])))\n",
    "print(\"Test Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of test points = {}\".format(len(test_previousFive_pickups), len(test_previousFive_pickups[0]), len(test_previousFive_pickups)*len(test_previousFive_pickups[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YMcnIAwYa2Z"
   },
   "outputs": [],
   "source": [
    "#taking 80% data as train data from each cluster\n",
    "train_lat = [i[:3567] for i in lat]\n",
    "train_lon = [i[:3567] for i in lon]\n",
    "train_weekDay = [i[:3567] for i in day_of_week]\n",
    "train_weighted_avg = [i[:3567] for i in predicted_pickup_values_list]\n",
    "train_TruePickups = [i[:3567] for i in TruePickups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brTXf7e2Ya2a"
   },
   "outputs": [],
   "source": [
    "#taking 20% data as test data from each cluster\n",
    "test_lat = [i[3567:] for i in lat]\n",
    "test_lon = [i[3567:] for i in lon]\n",
    "test_weekDay = [i[3567:] for i in day_of_week]\n",
    "test_weighted_avg = [i[3567:] for i in predicted_pickup_values_list]\n",
    "test_TruePickups = [i[3567:] for i in TruePickups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zF95iJyYa2b"
   },
   "outputs": [],
   "source": [
    "# convert from lists of lists of list to lists of list\n",
    "train_pickups = []\n",
    "test_pickups = []\n",
    "train_freq = []\n",
    "test_freq = []\n",
    "train_amp = []\n",
    "test_amp = []\n",
    "for i in range(30):\n",
    "    train_pickups.extend(train_previousFive_pickups[i])\n",
    "    test_pickups.extend(test_previousFive_pickups[i])\n",
    "    train_freq.extend(train_fourier_frequencies[i])\n",
    "    test_freq.extend(test_fourier_frequencies[i])\n",
    "    train_amp.extend(train_fourier_amplitudes[i])\n",
    "    test_amp.extend(test_fourier_amplitudes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ew2vV-fYa2d"
   },
   "outputs": [],
   "source": [
    "#stacking pickups,frequencies and amplitudes horizontally.\n",
    "# a = [1,2,3,4,5]\n",
    "# b = [6,7,8,9,10]\n",
    "# c = [11,12,13,14,15]\n",
    "# d = np.hstack((a, b, c))\n",
    "# d = array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
    "\n",
    "train_prevPickups_freq_amp = np.hstack((train_pickups, train_freq, train_amp))\n",
    "test_prevPickups_freq_amp = np.hstack((test_pickups, test_freq, test_amp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AaZyvS7ZYa2e"
   },
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data = {}. Number of columns till now = {}\".format(len(train_prevPickups_freq_amp), len(train_prevPickups_freq_amp[0])))\n",
    "print(\"Number of data points in test data = {}. Number of columns till now = {}\".format(len(test_prevPickups_freq_amp), len(test_prevPickups_freq_amp[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8UJZrSE4Ya2f"
   },
   "outputs": [],
   "source": [
    "# converting lists of lists into single list i.e flatten\n",
    "# a  = [[1,2,3,4],[4,6,7,8]]\n",
    "# print(sum(a,[]))\n",
    "# [1, 2, 3, 4, 4, 6, 7, 8]\n",
    "\n",
    "train_flat_lat = sum(train_lat, [])\n",
    "train_flat_lon = sum(train_lon, [])\n",
    "train_flat_weekDay = sum(train_weekDay, [])\n",
    "train_weighted_avg_flat = sum(train_weighted_avg, [])\n",
    "train_TruePickups_flat = sum(train_TruePickups, [])\n",
    "\n",
    "test_flat_lat = sum(test_lat, [])\n",
    "test_flat_lon = sum(test_lon, [])\n",
    "test_flat_weekDay = sum(test_weekDay, [])\n",
    "test_weighted_avg_flat = sum(test_weighted_avg, [])\n",
    "test_TruePickups_flat = sum(test_TruePickups, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJaAOn64Ya2f"
   },
   "outputs": [],
   "source": [
    "#train dataframe\n",
    "columns = ['ft_5','ft_4','ft_3','ft_2','ft_1', 'freq1', 'freq2','freq3','freq4','freq5', 'Amp1', 'Amp2', 'Amp3', 'Amp4', 'Amp5']\n",
    "Train_DF = pd.DataFrame(data = train_prevPickups_freq_amp, columns = columns)\n",
    "Train_DF[\"Latitude\"] = train_flat_lat\n",
    "Train_DF[\"Longitude\"] = train_flat_lon\n",
    "Train_DF[\"WeekDay\"] = train_flat_weekDay\n",
    "Train_DF[\"WeightedAvg\"] = train_weighted_avg_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3qS8zoU8Ya2h"
   },
   "outputs": [],
   "source": [
    "#test dataframe\n",
    "Test_DF = pd.DataFrame(data = test_prevPickups_freq_amp, columns = columns)\n",
    "Test_DF[\"Latitude\"] = test_flat_lat\n",
    "Test_DF[\"Longitude\"] = test_flat_lon\n",
    "Test_DF[\"WeekDay\"] = test_flat_weekDay\n",
    "Test_DF[\"WeightedAvg\"] = test_weighted_avg_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zl7ZTShCYa2i"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of train data = \"+str(Train_DF.shape))\n",
    "print(\"Shape of test data = \"+str(Test_DF.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cbOK_a4Ya2l"
   },
   "outputs": [],
   "source": [
    "Train_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3ZQxk-IYa2o"
   },
   "outputs": [],
   "source": [
    "Test_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RSlYPcpYa2p"
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-QczMg7Ya2r"
   },
   "outputs": [],
   "source": [
    "def lin_regression(train_data, train_true, test_data, test_true):\n",
    "    \n",
    "    #standardizing the data\n",
    "    train_std = StandardScaler().fit_transform(train_data)\n",
    "    test_std = StandardScaler().fit_transform(test_data)\n",
    "    \n",
    "    #hyper-paramater tuning\n",
    "    clf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\")\n",
    "    values = [10**-14, 10**-12, 10**-10, 10**-8, 10**-6, 10**-4, 10**-2, 10**0, 10**2, 10**4, 10**6]\n",
    "    hyper_parameter = {\"alpha\": values}\n",
    "    best_parameter = GridSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\n",
    "    best_parameter.fit(train_std, train_true)\n",
    "    alpha = best_parameter.best_params_[\"alpha\"]\n",
    "    \n",
    "    #applying linear regression with best hyper-parameter\n",
    "    clf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\", alpha = alpha)\n",
    "    clf.fit(train_std, train_true)\n",
    "    train_pred = clf.predict(train_std)\n",
    "    train_MAPE = mean_absolute_error(train_true, train_pred)/ (sum(train_true)/len(train_true))\n",
    "    train_MSE = mean_squared_error(train_true, train_pred)\n",
    "    test_pred = clf.predict(test_std)\n",
    "    test_MAPE = mean_absolute_error(test_true, test_pred)/ (sum(test_true)/len(test_true))\n",
    "    test_MSE = mean_squared_error(test_true, test_pred)\n",
    "    \n",
    "    return train_MAPE, train_MSE, test_MAPE, test_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIrVjjpSYa2s"
   },
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tE1_bFerYa2s"
   },
   "outputs": [],
   "source": [
    "def randomFor(train_data, train_true, test_data, test_true):\n",
    "    \n",
    "    #hyper-paramater tuning\n",
    "    values = [10, 40, 80, 150, 600, 800]\n",
    "    clf = RandomForestRegressor(n_jobs = -1)\n",
    "    hyper_parameter = {\"n_estimators\": values}\n",
    "    best_parameter = GridSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\n",
    "    best_parameter.fit(train_data, train_true)\n",
    "    estimators = best_parameter.best_params_[\"n_estimators\"]\n",
    "    \n",
    "    #applying random forest with best hyper-parameter\n",
    "    clf = RandomForestRegressor(n_estimators = estimators, n_jobs = -1)\n",
    "    clf.fit(train_data, train_true)\n",
    "    train_pred = clf.predict(train_data)\n",
    "    train_MAPE = mean_absolute_error(train_true, train_pred)/ (sum(train_true)/len(train_true))\n",
    "    train_MSE = mean_squared_error(train_true, train_pred)\n",
    "    test_pred = clf.predict(test_data)\n",
    "    test_MAPE = mean_absolute_error(test_true, test_pred)/ (sum(test_true)/len(test_true))\n",
    "    test_MSE = mean_squared_error(test_true, test_pred)\n",
    "    \n",
    "    return train_MAPE, train_MSE, test_MAPE, test_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6ifEsh9Ya2t"
   },
   "source": [
    "## XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRelFf_PYa2t"
   },
   "outputs": [],
   "source": [
    "def xgboost_reg(train_data, train_true, test_data, test_true):\n",
    "    #hyper-parameter tuning\n",
    "    hyper_parameter = {\"max_depth\":[1, 2, 3, 4], \"n_estimators\":[40, 80, 150, 600]}\n",
    "    clf = xgb.XGBRegressor()\n",
    "    best_parameter = GridSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\n",
    "    best_parameter.fit(train_data, train_true)\n",
    "    estimators = best_parameter.best_params_[\"n_estimators\"]\n",
    "    depth = best_parameter.best_params_[\"max_depth\"]\n",
    "    \n",
    "    #applying xgboost regressor with best hyper-parameter\n",
    "    clf = xgb.XGBRegressor(max_depth = depth, n_estimators = estimators)\n",
    "    clf.fit(train_data, train_true)\n",
    "    train_pred = clf.predict(train_data)\n",
    "    train_MAPE = mean_absolute_error(train_true, train_pred)/ (sum(train_true)/len(train_true))\n",
    "    train_MSE = mean_squared_error(train_true, train_pred)\n",
    "    test_pred = clf.predict(test_data)\n",
    "    test_MAPE = mean_absolute_error(test_true, test_pred)/ (sum(test_true)/len(test_true))\n",
    "    test_MSE = mean_squared_error(test_true, test_pred)\n",
    "    \n",
    "    return train_MAPE, train_MSE, test_MAPE, test_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5tHnz2DWYa2v"
   },
   "outputs": [],
   "source": [
    "trainMAPE_lr, trainMSE_lr, testMAPE_lr, testMSE_lr = lin_regression(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "# trainMAPE_rf, trainMSE_rf, testMAPE_rf, testMSE_rf = randomFor(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "trainMAPE_xgb, trainMSE_xgb, testMAPE_xgb, testMSE_xgb = xgboost_reg(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04QT8jy2Ya2w"
   },
   "source": [
    "# 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMA9fBzQYa2w"
   },
   "outputs": [],
   "source": [
    "error_table_regressions = pd.DataFrame(columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TestMAPE(%)\", \"TestMSE\"])\n",
    "\n",
    "error_table_regressions = error_table_regressions.append(pd.DataFrame([[\"Linear Regression\", trainMAPE_lr*100, trainMSE_lr, testMAPE_lr*100, testMSE_lr ]], columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TestMAPE(%)\", \"TestMSE\"]))\n",
    "# error_table_regressions = error_table_regressions.append(pd.DataFrame([[\"Random Forest Regression\", trainMAPE_rf*100, trainMSE_rf, testMAPE_rf*100, testMSE_rf]], columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TestMAPE(%)\", \"TestMSE\"]))\n",
    "error_table_regressions = error_table_regressions.append(pd.DataFrame([[\"XGBoost Regressor\", trainMAPE_xgb*100, trainMSE_xgb, testMAPE_xgb*100, testMSE_xgb]], columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TestMAPE(%)\", \"TestMSE\"]))\n",
    "error_table_regressions.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDfHnoGIYa2x"
   },
   "outputs": [],
   "source": [
    "error_table_regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05J9XCSlYa2z"
   },
   "source": [
    "### From the above result we have following observations:\n",
    "* <b>1. The difference between train error and test error of random forest regressor is high, which clearly shows that random forest regressor is overfitting. Therefore, we are discarding random forest regressor.</b>\n",
    "* <b>2. The best model with lowest train and test error is XGBoost Regressor.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vZovQ6xYa20"
   },
   "outputs": [],
   "source": [
    "Final_Table = pd.DataFrame(columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"])\n",
    "\n",
    "# Final_Table = Final_Table.append(pd.DataFrame([[\"Simple Moving Average Ratios\", mape1*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "# Final_Table = Final_Table.append(pd.DataFrame([[\"Simple Moving Average Predictions\", mape2*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "# Final_Table = Final_Table.append(pd.DataFrame([[\"Weighted Moving Average Ratios\", mape3*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "Final_Table = Final_Table.append(pd.DataFrame([[\"Weighted Moving Average Predictions\", mape4*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "# Final_Table = Final_Table.append(pd.DataFrame([[\"Exponential Weighted Moving Average Ratios\", mape5*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "# Final_Table = Final_Table.append(pd.DataFrame([[\"Exponential Weighted Moving Average Predictions\", mape6*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "Final_Table = Final_Table.append(pd.DataFrame([[\"Linear Regression\", testMAPE_lr*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "# Final_Table = Final_Table.append(pd.DataFrame([[\"Random Forest Regressor\", testMAPE_rf*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "Final_Table = Final_Table.append(pd.DataFrame([[\"XGBoost Regressor\", testMAPE_xgb*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "\n",
    "Final_Table.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rp_iL9D5Ya21"
   },
   "outputs": [],
   "source": [
    "ax = Final_Table.plot(x = \"Model\", kind = \"bar\", figsize = (12, 8), grid = True, fontsize = 15)\n",
    "ax.set_title(\"Test MAPE of all Models\", fontsize = 25)\n",
    "ax.set_ylabel(\"Mean Absolute Percentage Error(%)\", fontsize = 15)\n",
    "\n",
    "for i in ax.patches:   #ax.patches is an array which gives x position, y position, width of a bar graphs.\n",
    "    ax.text(i.get_x()-.05, i.get_height()+0.19, str(round(i.get_height(), 2))+'%', fontsize=14, color='black')\n",
    "#     ax.text(x, y, annotate_text, font_size, color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_DDPrOnYa22"
   },
   "outputs": [],
   "source": [
    "Final_Table.style.highlight_min(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mr7-HIn5Ya23"
   },
   "source": [
    "# So, far our best model is XGBoost Regressor with Test MAPE: 11.57%"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eOiXk_l_Yayb",
    "I99QDXT1Yayf",
    "L8lBioAWYayg",
    "Wq3fwjGGYayk",
    "u2dzwaFQYayo",
    "BSp4PufHYayx",
    "mxblmT1sYayy",
    "49fbtQPIYazC",
    "LBBC4QuRYazJ",
    "lC2snNshYazb",
    "DB-_UcXoYazi",
    "2xI9pXCpYazj",
    "Nj8OPjqMYaz1",
    "our9tR44Ya0I",
    "3pWO_BG0Ya0J",
    "xMzuG2gtYa0P",
    "vIK7yxfiYa0U",
    "z_8n46AbYa0b",
    "UJltlrztYa0e",
    "Vt6yhFcGYa0j",
    "nJPFXlnAYa0m",
    "EsOBbcPHYa0p",
    "ntvadQLOYa02",
    "0_IStlLrYa0_",
    "5sZ3nqTlYa1b",
    "lOEj57PqYa1h",
    "LnW1I9uKYa1n",
    "JrIIZMQUYa1v",
    "m10MaQdWYa11",
    "P-ryPltqYa19",
    "YgcjmewbYa2A",
    "1ZZ9LHRrYa2D",
    "unJ_-uRTYa2L",
    "-RSlYPcpYa2p",
    "SIrVjjpSYa2s",
    "z6ifEsh9Ya2t",
    "05J9XCSlYa2z"
   ],
   "name": "Taxi-Demand-Prediction-NYC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
