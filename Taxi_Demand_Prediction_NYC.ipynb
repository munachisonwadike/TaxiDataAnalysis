{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ah44kDXQYaxt"
   },
   "source": [
    "# Taxi Demand Prediction - New York City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPU9O3YyYaxv"
   },
   "source": [
    "## Business/Real World Problem\n",
    "For a given location in New York City, our goal is to <b>predict the average speed of a pickup from that given location</b>. \n",
    "\n",
    "As Taxi drivers in New York travel faster than 12miles/hr, they are paid more since their charge by the distance, in addition to any surcharges. As they go slower than 12miles/hr, they are charged by time. As a result, they benefit by going slower since it gaurantees a fixed income given that there is already traffic. The estimates of pickup speed could be transferred to their smartphones in real time.\n",
    "\n",
    "There are two kinds of taxi for which we have data available\n",
    "\n",
    "Yellow Taxi: Exclusively through street-hails. Can access all parts of New York\n",
    "Green Taxi: Street Hail Livery (SHL)- Limited to certain part of New York\n",
    "\n",
    "We are considering only the yellow taxis for the time period of ... . We have used ... data to make prediction for ... data.</b>\n",
    "\n",
    "Credits to GauravTheP for his brilliant work on predicting number of pickups per location in New York, which we built upon for this work -> https://github.com/gauravtheP/Taxi-Demand-Prediction-New-York-City/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVpsSf-dZE6S"
   },
   "outputs": [],
   "source": [
    "# !pip install -q gpxpy\n",
    "# !pip install -q sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ty2UnwFxZkvN"
   },
   "outputs": [],
   "source": [
    "# !pip install -q sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMGrXvSrYayH"
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import folium\n",
    "import gpxpy.geo\n",
    "from datetime import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve,GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "\n",
    "\n",
    "# from numba import jit, cuda \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GCDjEgwYayJ"
   },
   "outputs": [],
   "source": [
    "# !wget https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2015-01.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ry_bqKiRfoX9"
   },
   "outputs": [],
   "source": [
    "# !wget https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4F_MfpjibBFf",
    "outputId": "63abaa46-2429-4a6d-dd24-1ce32c434c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML_Project_3.ipynb\t\t  yellow_tripdata_2015-01.csv\r\n",
      "README.md\t\t\t  yellow_tripdata_2016-01.csv\r\n",
      "Taxi_Demand_Prediction_NYC.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLe871faalIm"
   },
   "outputs": [],
   "source": [
    "data_2015 = dd.read_csv(\"yellow_tripdata_2015-01.csv\")\n",
    "# data_2015 = pd.read_csv(\"yellow_tripdata_2015-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "JLit0F3jYayM",
    "outputId": "550a5716-7301-4b5c-d170-bbe3b79faeea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-15 19:05:39</td>\n",
       "      <td>2015-01-15 19:23:42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-73.993896</td>\n",
       "      <td>40.750111</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.974785</td>\n",
       "      <td>40.750618</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:53:28</td>\n",
       "      <td>1</td>\n",
       "      <td>3.30</td>\n",
       "      <td>-74.001648</td>\n",
       "      <td>40.724243</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.994415</td>\n",
       "      <td>40.759109</td>\n",
       "      <td>1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:38</td>\n",
       "      <td>2015-01-10 20:43:41</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-73.963341</td>\n",
       "      <td>40.802788</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.951820</td>\n",
       "      <td>40.824413</td>\n",
       "      <td>2</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:35:31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-74.009087</td>\n",
       "      <td>40.713818</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004326</td>\n",
       "      <td>40.719986</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-10 20:33:39</td>\n",
       "      <td>2015-01-10 20:52:58</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>-73.971176</td>\n",
       "      <td>40.762428</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.004181</td>\n",
       "      <td>40.742653</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2015-01-15 19:05:39   2015-01-15 19:23:42                1   \n",
       "1         1  2015-01-10 20:33:38   2015-01-10 20:53:28                1   \n",
       "2         1  2015-01-10 20:33:38   2015-01-10 20:43:41                1   \n",
       "3         1  2015-01-10 20:33:39   2015-01-10 20:35:31                1   \n",
       "4         1  2015-01-10 20:33:39   2015-01-10 20:52:58                1   \n",
       "\n",
       "   trip_distance  pickup_longitude  pickup_latitude  RateCodeID  \\\n",
       "0           1.59        -73.993896        40.750111           1   \n",
       "1           3.30        -74.001648        40.724243           1   \n",
       "2           1.80        -73.963341        40.802788           1   \n",
       "3           0.50        -74.009087        40.713818           1   \n",
       "4           3.00        -73.971176        40.762428           1   \n",
       "\n",
       "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
       "0                  N         -73.974785         40.750618             1   \n",
       "1                  N         -73.994415         40.759109             1   \n",
       "2                  N         -73.951820         40.824413             2   \n",
       "3                  N         -74.004326         40.719986             2   \n",
       "4                  N         -74.004181         40.742653             2   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0         12.0    1.0      0.5        3.25           0.0   \n",
       "1         14.5    0.5      0.5        2.00           0.0   \n",
       "2          9.5    0.5      0.5        0.00           0.0   \n",
       "3          3.5    0.5      0.5        0.00           0.0   \n",
       "4         15.0    0.5      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  \n",
       "0                    0.3         17.05  \n",
       "1                    0.3         17.80  \n",
       "2                    0.3         10.80  \n",
       "3                    0.3          4.80  \n",
       "4                    0.3         16.30  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "XeC7Q8XKYayP",
    "outputId": "81ff30a0-823e-47b3-8e2b-5c07e0667f7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
       "       'passenger_count', 'trip_distance', 'pickup_longitude',\n",
       "       'pickup_latitude', 'RateCodeID', 'store_and_fwd_flag',\n",
       "       'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount',\n",
       "       'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
       "       'improvement_surcharge', 'total_amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CHPOU2pLYayS",
    "outputId": "569ec0a3-b53a-4379-af83-5e36fc264779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns = 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of columns = \"+str(len(data_2015.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6QTdGm1YayU"
   },
   "outputs": [],
   "source": [
    "# data_2015.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A1acjBpBYayX",
    "outputId": "ce4ddf68-e719-4a50-85a5-b7f384bf5a7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.Scalar<series-..., dtype=int64>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2015[\"passenger_count\"].sum()\n",
    "# data_2015[\"passenger_count\"].sum().visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bpJBM_0zYayZ",
    "outputId": "add0fda6-4668-44dd-dfaa-faea7cdd49bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Passengers in January 2015 = 21437303\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of Passengers in January 2015 = \"+str(data_2015[\"passenger_count\"].sum(axis = 0).compute()))\n",
    "# print(\"Total Number of Passengers in January 2015 = \"+str(data_2015[\"passenger_count\"].sum(axis = 0))) #for pd instead of dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "2i7lRYScYayc",
    "outputId": "db37ac90-c908-48a5-ef0c-20641aeb668e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nan values = VendorID                 0\n",
      "tpep_pickup_datetime     0\n",
      "tpep_dropoff_datetime    0\n",
      "passenger_count          0\n",
      "trip_distance            0\n",
      "pickup_longitude         0\n",
      "pickup_latitude          0\n",
      "RateCodeID               0\n",
      "store_and_fwd_flag       0\n",
      "dropoff_longitude        0\n",
      "dropoff_latitude         0\n",
      "payment_type             0\n",
      "fare_amount              0\n",
      "extra                    0\n",
      "mta_tax                  0\n",
      "tip_amount               0\n",
      "tolls_amount             0\n",
      "improvement_surcharge    3\n",
      "total_amount             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nan values = {}\".format(data_2015.isnull().sum().compute()))\n",
    "# print(\"Number of nan values = {}\".format(data_2015.isnull().sum())) #for pd instead of dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aer4sWEqYaye"
   },
   "source": [
    "Since we will not be using \"improvement_surcharge\" in our task, so nan values will not affect us. So, we are leaving it as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq3fwjGGYayk"
   },
   "source": [
    "### Pickup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Wi0LgATYayl",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #detecting the pickups latitude and longitudes which are outside NYC.\n",
    "# outside_NYC = data_2015[((data_2015.pickup_latitude <= 40.5774) | (data_2015.pickup_longitude <= -74.15) | (data_2015.pickup_latitude >= 40.9176) | (data_2015.pickup_longitude >= -73.7004))]\n",
    "# #latitude at equator is 0. Above equator latitude increases and becomes 90 at north pole. Below equator latitude decreases and\n",
    "# #is negative and becomes -90 at south pole.\n",
    "# #Longitude is 0 at United Kingdom(UK). To the right of UK, longitude increases positively and to the left of UK longitude decrease\n",
    "# #and is negative.\n",
    "\n",
    "# m = folium.Map(location = [40.5774, -73.7004], tiles = \"Stamen Toner\")\n",
    "\n",
    "# outside_pickups = outside_NYC.head(25000)\n",
    "\n",
    "# for i,j in outside_pickups.iterrows():\n",
    "#     if j[\"pickup_latitude\"] != 0:\n",
    "#         folium.Marker([j[\"pickup_latitude\"], j[\"pickup_longitude\"]]).add_to(m)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mL31nTSoYayo"
   },
   "source": [
    "<b>Observation:</b>As you can see in the above map, there are many erroneous data points, which are in some other state, and some of them are even in Atlantic Ocean. All these erroneous data points will also be removed as a part of data cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2dzwaFQYayo"
   },
   "source": [
    "### Dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0kTwS_BYayp"
   },
   "outputs": [],
   "source": [
    "# #detecting the dropoff latitude and longitudes which are outside NYC.\n",
    "# outside_NYC = data_2015[((data_2015.dropoff_latitude <= 40.5774) | (data_2015.dropoff_longitude <= -74.15) | (data_2015.dropoff_latitude >= 40.9176) | (data_2015.dropoff_longitude >= -73.7004))]\n",
    "# #latitude at equator is 0. Above equator latitude increases and becomes 90 at north pole. Below equator latitude decreases and\n",
    "# #is negative and becomes -90 at south pole.\n",
    "# #Longitude is 0 at United Kingdom(UK). To the right of UK, longitude increases positively and to the left of UK longitude decrease\n",
    "# #and is negative.\n",
    "\n",
    "# m = folium.Map(location = [40.5774, -73.7004], tiles = \"Stamen Toner\")\n",
    "\n",
    "# outside_dropoff = outside_NYC.head(25000)\n",
    "\n",
    "# for i,j in outside_dropoff.iterrows():\n",
    "#     if j[\"dropoff_latitude\"] != 0:\n",
    "#         folium.Marker([j[\"dropoff_latitude\"], j[\"dropoff_longitude\"]]).add_to(m)\n",
    "# m\n",
    "# #documentation of folium: https://python-visualization.github.io/folium/docs-v0.6.0/quickstart.html#Getting-Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZiDc4-jHYays"
   },
   "source": [
    "<b>Observation:</b>As you can see in the above map, there are many erroneous data points, which are in some other state, and some of them are even in Atlantic Ocean. All these erroneous data points will also be removed as a part of data cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MX0dEyjLYays"
   },
   "source": [
    "### Creating new Dataframe with Trip duration and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLvz5tRbYayt"
   },
   "outputs": [],
   "source": [
    "def timeToUnix(t):\n",
    "    #we have a time in the format \"YYYY-MM-DD HH:MM:SS\", which is a string\n",
    "    change = datetime.strptime(t, \"%Y-%m-%d %H:%M:%S\") #this will convert the String time into datetime format\n",
    "    t_tuple = change.timetuple() #this will convert the datetime formatted time into structured time\n",
    "    return time.mktime(t_tuple) + 3600  #this will convert structured time into unix-time.\n",
    "    #Now why, I have added 3600 in the above unix times. NOW, UNIX TIMESTAMP MEANS HOW MANY SECONDS HAVE ELAPSED SINCE 1 JAN 1970\n",
    "    #(EPOCH) CALCULATED FROM THE REFERENCE OF GMT. I HAVE MADE THIS PROJECT IN GERMANY WHICH IS 1HR/3600SECS AHEAD OF GMT TIME, \n",
    "    #AND HERE \"time.mktime()\" FUNCTION RETURNS UNIX TIMESTAMP FROM THE REFERENCE OF LOCAL TIME. SO, THEREFORE, IN ORDER TO \n",
    "    #COMPENSATE FOR 1HR AHEAD, \"time.mktime\" SUBTRACTED 3600 SECONDS MEANS 1HR FROM UNIX TIME STAMP IN ORDER TO CATER TO \n",
    "    #LOCAL TIME. SO, THEREFORE, IF WE WANT OUR UNIX TIME TO BE EXACTLY EQUAL TO GMT TIME, WE HAVE TO ADD 3600 SECONDS \n",
    "    #MEANS 1HR TO UNIX TIME. lET SAY AT 12:00AM ON 1st JAN 1970, TIME ELAPSED AT GMT IS 0, THE TIME ELAPSED IN GERMANY IS \n",
    "    #3600SEC. NOW ON 1st JAN 2015, ELASPED SECONDS AT GMT IS 'X', SO THE EQUIVALENT ELAPSED SECONDS IN GERMANY WILL BE X+3600. \n",
    "    #NOW \"time.mktime()\" SUBTRACT THIS 3600 EXTRA IN GERMAN TIME WHICH WE HAVE TO ADD IN ORDER TO MAKE IT EQUAL TO GMT.\n",
    "\n",
    "def dfWithTripTimes(df):\n",
    "    startTime = datetime.now()\n",
    "    duration = df[[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]].compute()\n",
    "    #duration = df[[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]] #for pd instead of dd\n",
    "\n",
    "    pickup_time = [timeToUnix(pkup) for pkup in duration[\"tpep_pickup_datetime\"].values]\n",
    "    dropoff_time = [timeToUnix(drpof) for drpof in duration[\"tpep_dropoff_datetime\"].values]\n",
    "#     trip_duration = []\n",
    "#     for xy in zip(dropoff_time, pickup_time):\n",
    "#         trip_duration.append(xy[0] - xy[1])\n",
    "    trip_duration = (np.array(dropoff_time) - np.array(pickup_time))/float(60)  #trip duration in minutes\n",
    "    \n",
    "    NewFrame = df[['passenger_count','trip_distance','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','total_amount']].compute()\n",
    "#     NewFrame = df[['passenger_count','trip_distance','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','total_amount']] #for pd instead of dd\n",
    "    NewFrame[\"trip_duration\"] = trip_duration\n",
    "    NewFrame[\"pickup_time\"] = pickup_time\n",
    "    NewFrame[\"speed\"] = (NewFrame[\"trip_distance\"]/NewFrame[\"trip_duration\"])*60  #speed in miles/hr\n",
    "    \n",
    "    print(\"Time taken for creation of dataframe is {}\".format(datetime.now() - startTime))\n",
    "    return NewFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djqvumpjYazK"
   },
   "outputs": [],
   "source": [
    "def changingLabels(num):\n",
    "    if num < 10**3:\n",
    "        return num\n",
    "    elif num>=10**3 and num < 10**6:\n",
    "        return str(num/10**3)+\"k\"\n",
    "    elif num>=10**6 and num < 10**9:\n",
    "        return str(num/10**6) + \"M\"\n",
    "    else:\n",
    "        return str(num/10**9) + \"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcjoNGmjYa0p"
   },
   "outputs": [],
   "source": [
    "# 1420070400 : 2015-01-01 00:00:00   (Equivalent unix time)\n",
    "# 1451606400 : 2016-01-01 00:00:00   (Equivalent unix time)\n",
    "\n",
    "def pickup_10min_bins(dataframe, month, year):\n",
    "    pickupTime = dataframe[\"pickup_time\"].values\n",
    "    unixTime = [1420070400, 1451606400]\n",
    "    unix_year = unixTime[year-2015]\n",
    "    time_10min_bin = [int((i - unix_year)/600) for i in pickupTime]\n",
    "    dataframe[\"time_bin\"] = np.array(time_10min_bin)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation for Januray 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "X_apoY9LYazv",
    "outputId": "7523953b-9c57-4880-9db0-83de0f1aab0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARATION OF JANUARY 2016 DATA.\n",
      "-----------------------------------\n",
      "Number of columns = 19\n",
      "-----------------------------------\n",
      "Time taken for creation of dataframe is 0:05:25.018274\n",
      "New Frame for Jan 2015 creation done\n",
      "-----------------------------------\n",
      "Trip Duration Outliers removed\n",
      "-----------------------------------\n",
      "Speed Outliers removed\n",
      "-----------------------------------\n",
      "Trip Distance Outliers removed\n",
      "-----------------------------------\n",
      "Total Amount Outliers removed\n",
      "-----------------------------------\n",
      "Pickups outside of NYC are removed\n",
      "-----------------------------------\n",
      "Dropoffs outside of NYC are removed\n",
      "-----------------------------------\n",
      "Fraction of cleaned points 0.9680387130396095\n",
      "Total number of outliers and erroneous points removed =  407474\n",
      "Pickup Clusters are assigned\n",
      "-----------------------------------\n",
      "Pickup time bins are assigned\n",
      "-----------------------------------\n",
      "There should be ((24*60)/10)*31 unique 10 minute time bins for the month of January 2015:  4464\n",
      "Pickup cluster and time bins are grouped.\n",
      "-----------------------------------\n",
      "Done...\n",
      "-----------------------------------\n",
      "Fraction of Total data left = 0.9680387130396095\n",
      "Total Number of outliers removed = 407474\n",
      "-----------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'startTime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e2d1d9d26530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total Time taken for execution of Jan 2016 data = \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'startTime' is not defined"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "\n",
    "print(\"PREPARATION OF JANUARY 2015 DATA.\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Number of columns = \"+str(len(data_2015.columns)))\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame = dfWithTripTimes(data_2015)\n",
    "print(\"New Frame for Jan 2015 creation done\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "# Observations:Here, 0th percentile value of trip duration is negative(weird), also 99th percentile \n",
    "# value of trip duration is 46.75min, but 100th percentile value is 548555.633min(weird). \n",
    "# 0th and 100th percentile values are certainly an erroneous points.\n",
    "\n",
    "# According to NYC Taxi and Limousine Commission regulations, the maximum allowed trip duration \n",
    "# in a 24hrs interval is 12 hrs.Â¶\n",
    "\n",
    "new_frame_cleaned = new_frame[(new_frame.trip_duration>1) & (new_frame.trip_duration<720)]\n",
    "print(\"Trip Duration Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "# Observations: Here, 100th percentile value of a speed is 192 Million miles/hr which is (BIZZARE). \n",
    "# Furthermore, 99.9th percentile value of speed is 45.31miles/hr. So, we are removing all the data points \n",
    "# where speed is greater than 45.31miles/hr.\n",
    "new_frame_cleaned = new_frame_cleaned[(new_frame_cleaned.speed>0) & (new_frame_cleaned.speed<45.31)]\n",
    "print(\"Speed Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "\n",
    "# Observation: Here, 99.9th percentile of trip distance is 22.58miles, however, \n",
    "# 100th percentile value is 258.9miles, which is very high. So, we are removing all \n",
    "# the data points where trip distance is greater than 23miles.\n",
    "new_frame_cleaned = new_frame_cleaned[(new_frame_cleaned.trip_distance>0) & (new_frame_cleaned.trip_distance<23)]\n",
    "print(\"Trip Distance Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "\n",
    "# Observation:Here, 99.9th percentile fare amount of a trip is 86.6. \n",
    "# However, 100th percentile of a fare amount is 3Million which is bizzare.\n",
    "# Therefore, we have removedall the data points where fare amount is more than \n",
    "# 99.9th percentile value.\n",
    "new_frame_cleaned = new_frame_cleaned[(new_frame_cleaned.total_amount>0) & (new_frame_cleaned.total_amount<86.6)]\n",
    "print(\"Total Amount Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "# Removing Pickups outside NYC\n",
    "new_frame_cleaned = new_frame_cleaned[(((new_frame_cleaned.pickup_latitude >= 40.5774) & (new_frame_cleaned.pickup_latitude <= 40.9176)) & ((new_frame_cleaned.pickup_longitude >= -74.15) & (new_frame_cleaned.pickup_longitude <= -73.7004)))]\n",
    "print(\"Pickups outside of NYC are removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "# Removing Dropoffs outside NYC\n",
    "new_frame_cleaned = new_frame_cleaned[(((new_frame_cleaned.dropoff_latitude >= 40.5774) & (new_frame_cleaned.dropoff_latitude <= 40.9176)) & ((new_frame_cleaned.dropoff_longitude >= -74.15) & (new_frame_cleaned.dropoff_longitude <= -73.7004)))]\n",
    "print(\"Dropoffs outside of NYC are removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "\n",
    "# Fraction of points left after removing all the erroneous points and outlier points. \n",
    "# Points where pickups and dropoffs are outside of NYC are also removed.\n",
    "print(\"Fraction of cleaned points\",str(new_frame_cleaned.shape[0]/new_frame.shape[0]))\n",
    "\n",
    "print(\"Total number of outliers and erroneous points removed = \",str(new_frame.shape[0] - new_frame_cleaned.shape[0]))\n",
    "\n",
    "#Clustering -setup\n",
    "coord = new_frame_cleaned[[\"pickup_latitude\", \"pickup_longitude\"]].values\n",
    "regions = MiniBatchKMeans(n_clusters = 30, batch_size = 10000).fit(coord)\n",
    "\n",
    "#Clustering-assignment\n",
    "new_frame_cleaned[\"pickup_cluster\"] = regions.predict(new_frame_cleaned[[\"pickup_latitude\", \"pickup_longitude\"]])\n",
    "print(\"Pickup Clusters are assigned\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "# Time Binning\n",
    "jan_2015_data = pickup_10min_bins(new_frame_cleaned, 1, 2015)\n",
    "print(\"Pickup time bins are assigned\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "jan_2015_data.head()\n",
    "print(\"There should be ((24*60)/10)*31 unique 10 minute time bins for the month of January 2015: \", str(len(np.unique(jan_2015_data[\"time_bin\"]))))\n",
    "\n",
    "#grouping by pickup cluster then time bin within pickup cluster\n",
    "jan_2015_timeBin_groupBy = jan_2015_data[[\"pickup_cluster\", \"time_bin\", \"trip_distance\"]].groupby(by = [\"pickup_cluster\", \"time_bin\"]).count()\n",
    "print(\"Pickup cluster and time bins are grouped.\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "jan_2015_timeBin_groupBy.head()\n",
    "\n",
    "print(\"Done...\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Fraction of Total data left = \"+str(new_frame_cleaned.shape[0]/new_frame.shape[0]))\n",
    "print(\"Total Number of outliers removed = \"+str(new_frame.shape[0] - new_frame_cleaned.shape[0]))\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Total Time taken for execution of Jan 2016 data = \"+str(datetime.now() - startTime))\n",
    "print(\"-\"*35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49fbtQPIYazC"
   },
   "source": [
    "#### Plot speed after removing outliers and erroneous points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMY1rIUeYazI"
   },
   "source": [
    "<b>Observation:</b> PDF plot shows that most trip speeds are just above 10mls/hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "6l5YeJOsYazG",
    "outputId": "f6eba8e3-4091-4868-b1aa-9d55bc7d9533"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (12,8))\n",
    "# sns.kdeplot(new_frame_cleaned[\"trip_duration\"].values, shade = True, cumulative = False)\n",
    "# plt.tick_params(labelsize = 20)\n",
    "# plt.xlabel(\"Trip Duration\", fontsize = 20)\n",
    "# plt.title(\"PDF of Trip Duration\", fontsize = 20)\n",
    "# plt.show()\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.kdeplot(new_frame_cleaned[\"speed\"].values, shade = True, cumulative = False)\n",
    "plt.tick_params(labelsize = 20)\n",
    "plt.xlabel(\"Trip Speed\", fontsize = 20)\n",
    "plt.title(\"PDF of Trip Speed\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lC2snNshYazb"
   },
   "source": [
    "#### Box plot of speed after removing outliers and erroneous points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (10,6))\n",
    "# ax = sns.boxplot(\"speed\", data = new_frame_cleaned, orient = \"v\")\n",
    "\n",
    "# plt.tick_params(labelsize = 20)\n",
    "# plt.ylabel(\"Speed(Miles/hr)\", fontsize = 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JQHGu07-Yaze",
    "outputId": "2b826045-b2b2-4537-e7d0-a8a8c341d363"
   },
   "outputs": [],
   "source": [
    "Average_speed = sum(new_frame_cleaned.speed)/len(new_frame_cleaned.speed)\n",
    "print(\"Average Speed of Taxis around NYC = \"+str(Average_speed))\n",
    "# The avg speed in Newyork speed is 12.45miles/hr, so a cab driver can travel 2 miles per 10min on avg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hDdoBwmEYazg",
    "outputId": "8532c248-f32a-44e1-dfa1-ce34d1f68c9a"
   },
   "outputs": [],
   "source": [
    "print(\"Speed of Taxis around NYC per 10 minutes = \"+str(Average_speed/6)+\" per 10 minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2xI9pXCpYazj"
   },
   "source": [
    "#### Trip Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWmFQ5yFYa0P"
   },
   "source": [
    "<b>Observation:</b> Most of the pickups are concentrated in and around Manhattan district of New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KkToSz6-Ya0M",
    "outputId": "8a2e8278-1569-4ba5-e20c-6440d3d7a2c5"
   },
   "outputs": [],
   "source": [
    "# m = folium.Map(location = [40.9176, -73.7004], tiles = \"Stamen Toner\")\n",
    "\n",
    "# pickups_within_NYC = new_frame_cleaned.sample(n = 500)\n",
    "\n",
    "# for i,j in pickups_within_NYC.iterrows():\n",
    "#     folium.Marker([j[\"pickup_latitude\"], j[\"pickup_longitude\"]]).add_to(m)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yH_TrP8_Ya0T"
   },
   "source": [
    "<b>Observation:</b> Most of the dropoffs are concentrated in and around Manhattan district of New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mN0u70z5Ya0R",
    "outputId": "a7e887cc-1146-4fe8-a551-e19c84550b7f"
   },
   "outputs": [],
   "source": [
    "# m = folium.Map(location = [40.9176, -73.7004], tiles = \"Stamen Toner\")\n",
    "\n",
    "# dropoff_within_NYC = new_frame_cleaned.sample(n = 500)\n",
    "\n",
    "# for i,j in dropoff_within_NYC.iterrows():\n",
    "#     folium.Marker([j[\"dropoff_latitude\"], j[\"dropoff_longitude\"]]).add_to(m)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt6yhFcGYa0j"
   },
   "source": [
    "#### Plotting cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZPKchMIxYa0j",
    "outputId": "46b97749-abd9-4b3c-a01e-ec3963e7326b"
   },
   "outputs": [],
   "source": [
    "# centerOfRegions = regions.cluster_centers_\n",
    "# noOfClusters = len(centerOfRegions)\n",
    "# m = folium.Map(location = [40.9176, -73.7004], tiles = \"Stamen Toner\")\n",
    "\n",
    "# for i in range(noOfClusters):\n",
    "#     folium.Marker([centerOfRegions[i][0], centerOfRegions[i][1]], popup = (str(np.round(centerOfRegions[i][0], 2))+\", \"+str(np.round(centerOfRegions[i][1], 2)))).add_to(m)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nJPFXlnAYa0m"
   },
   "source": [
    "#### Plotting Regions in NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "Uy2tPMZ3Ya0n",
    "outputId": "ebc493fa-1b9f-4ab6-eb18-92f725ef489c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NYC_latitude_range = (40.5774, 40.9176)\n",
    "NYC_Longitude_range = (-74.15, -73.7004)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1.5,1.5])\n",
    "ax.scatter(x = new_frame_cleaned.pickup_longitude.values[:70000], y = new_frame_cleaned.pickup_latitude.values[:70000], c = new_frame_cleaned.pickup_cluster.values[:70000], cmap = \"Paired\", s = 5)\n",
    "ax.set_xlim(-74.10, -73.72)\n",
    "ax.set_ylim(40.5774, 40.9176)\n",
    "ax.set_title(\"Regions in New York City\")\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "plt.show()\n",
    "#Longitude values vary from left to right i.e., horizontally\n",
    "#Latitude values vary from top to bottom means i.e., vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntvadQLOYa02"
   },
   "source": [
    "## Data Preparation for Januray 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "lHoWeRIzYa03",
    "outputId": "d6d13601-2b77-4558-c785-e824bd7aa277"
   },
   "outputs": [],
   "source": [
    "# Up till now we cleaned data and prepared data for the month of Jan 2015.\n",
    "\n",
    "# now doing the same operations for the month of Jan 2016.\n",
    "\n",
    "# 1. Get the dataframe which includes only required colums.\n",
    "# 2. Add trip_duration, speed, unix time stamp of pickup_time.\n",
    "# 4. Remove the outliers based on trip_duration, speed, trip_distance, total_amount.\n",
    "# 5. Remove all the points where pickup and dropoff are outside of New York City area.\n",
    "# 6. Add pickup_cluster to each data point.\n",
    "# 7. Add time_bin (index of 10min intravel to which that trip belongs to).\n",
    "# 8. Group by data, based on 'pickup_cluster' and 'time_bin'\n",
    "startTime = datetime.now()\n",
    "frame_2016 = dd.read_csv(\"yellow_tripdata_2016-01.csv\")\n",
    "# frame_2016 = pd.read_csv(\"yellow_tripdata_2016-01.csv\")\n",
    "\n",
    "print(\"PREPARATION OF JANUARY 2016 DATA.\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Number of columns = \"+str(len(frame_2016.columns)))\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame2 = dfWithTripTimes(frame_2016)\n",
    "print(\"New Frame for Jan 2016 creation done\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame2[(new_frame2.trip_duration>1) & (new_frame2.trip_duration<720)]\n",
    "print(\"Trip Duration Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame_cleaned2[(new_frame_cleaned2.speed>0) & (new_frame_cleaned2.speed<45.31)]\n",
    "print(\"Speed Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame_cleaned2[(new_frame_cleaned2.trip_distance>0) & (new_frame_cleaned2.trip_distance<23)]\n",
    "print(\"Trip Distance Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame_cleaned2[(new_frame_cleaned2.total_amount>0) & (new_frame_cleaned2.total_amount<86.6)]\n",
    "print(\"Total Amount Outliers removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame_cleaned2[(((new_frame_cleaned2.pickup_latitude >= 40.5774) & (new_frame_cleaned2.pickup_latitude <= 40.9176)) & ((new_frame_cleaned2.pickup_longitude >= -74.15) & (new_frame_cleaned2.pickup_longitude <= -73.7004)))]\n",
    "print(\"Pickups outside of NYC are removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2 = new_frame_cleaned2[(((new_frame_cleaned2.dropoff_latitude >= 40.5774) & (new_frame_cleaned2.dropoff_latitude <= 40.9176)) & ((new_frame_cleaned2.dropoff_longitude >= -74.15) & (new_frame_cleaned2.dropoff_longitude <= -73.7004)))]\n",
    "print(\"Dropoffs outside of NYC are removed\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "new_frame_cleaned2[\"pickup_cluster\"] = regions.predict(new_frame_cleaned2[[\"pickup_latitude\", \"pickup_longitude\"]])\n",
    "print(\"Pickup Clusters are assigned\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "jan_2016_data = pickup_10min_bins(new_frame_cleaned2, 1, 2016)\n",
    "print(\"Pickup time bins are assigned\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "jan_2016_timeBin_groupBy = jan_2016_data[[\"pickup_cluster\", \"time_bin\", \"trip_distance\"]].groupby(by = [\"pickup_cluster\", \"time_bin\"]).count()\n",
    "print(\"Pickup cluster and time bins are grouped.\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Done...\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Fraction of Total data left = \"+str(new_frame_cleaned2.shape[0]/new_frame2.shape[0]))\n",
    "print(\"Total Number of outliers removed = \"+str(new_frame2.shape[0] - new_frame_cleaned2.shape[0]))\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Total Time taken for execution of Jan 2016 data = \"+str(datetime.now() - startTime))\n",
    "print(\"-\"*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "_rQPn7YkYa04",
    "outputId": "a062274e-94d1-49fd-a74a-6bb07d1b5129"
   },
   "outputs": [],
   "source": [
    "jan_2016_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "eVKuiT1AYa06",
    "outputId": "8191c34c-2738-4199-9862-1c56ccf0dfeb"
   },
   "outputs": [],
   "source": [
    "jan_2016_timeBin_groupBy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "N5J3TEpWYa07",
    "outputId": "916e5a80-3404-4315-ad52-7b515d47ca25"
   },
   "outputs": [],
   "source": [
    "jan_2015_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "EsntCvxBYa09",
    "outputId": "c532666c-0427-414f-c8b3-abc64b0fd852",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jan_2015_timeBin_groupBy.head()\n",
    "# jan_2015_timeBin_groupBy[\"trip_distance\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_IStlLrYa0_"
   },
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16N7Yww0Ya1B"
   },
   "outputs": [],
   "source": [
    "# Gets the unique time bins where pickup values are present for each region.\n",
    "\n",
    "# for each cluster region we will collect all the indices of 10min intervals in which pickups happened.\n",
    "# we got an observation that there are some time bins that doesn't have any pickups.\n",
    "\n",
    "def getUniqueBinsWithPickups(dataframe):\n",
    "    values = []\n",
    "    for i in range(30):          #we have total 30 clusters\n",
    "        cluster_id = dataframe[dataframe[\"pickup_cluster\"] == i]\n",
    "        unique_clus_id = list(set(cluster_id[\"time_bin\"]))\n",
    "        unique_clus_id.sort()   #inplace sorting\n",
    "        values.append(unique_clus_id)\n",
    "        print( dataframe[\"pickup_cluster\"]])\n",
    "#         print(dataframe[\"pickup_cluster\"]==i)\n",
    "#         print(dataframe[dataframe[\"pickup_cluster\"]==i])\n",
    "\n",
    "\n",
    "    return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hSNMZ5P8Ya1C",
    "outputId": "17439507-964a-4af4-dd76-f8aca38dda30"
   },
   "outputs": [],
   "source": [
    "#now for Jan-2015, we have to find out, how many time_bins are there where there is no pickup in any of the cluster region\n",
    "unique_binswithPickup_Jan_2015 = getUniqueBinsWithPickups(jan_2015_data)\n",
    "for i in range(30):             #we have total 30 clusters\n",
    "    print(\"For cluster ID {}, total number of time bins with no pickup in this clutser region is {}\".format(i, (4464 - len(unique_binswithPickup_Jan_2015[i]))))\n",
    "    print(\"-\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xdd3nk7bYa1G"
   },
   "source": [
    "There are two ways to fill up these values:\n",
    "\n",
    "* Fill the missing value with 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Abou369JYa1G"
   },
   "outputs": [],
   "source": [
    "# Fill the missing value with 0's.\n",
    "def fillMissingWithZero(numberOfPickups, correspondingTimeBin):\n",
    "    ind = 0\n",
    "    smoothed_regions = []\n",
    "    for c in range(0, 30):\n",
    "        smoothed_bins = []\n",
    "        for t in range(4464):    #there are total 4464 time bins in both Jan-2015 & Feb-2016.\n",
    "            if t in correspondingTimeBin[c]:   #if a time bin is present in \"correspondingTimeBin\" in cluster 'c', \n",
    "            #then it means there is a pickup, in this case, we are simply adding number of pickups, else we are adding 0.\n",
    "                smoothed_bins.append(numberOfPickups[ind])\n",
    "                ind += 1\n",
    "            else:\n",
    "                smoothed_bins.append(0)\n",
    "        smoothed_regions.extend(smoothed_bins)\n",
    "    return smoothed_regions\n",
    "#above function performs the operation in this way: if in any cluster if there is no pickup in any of the 4464 time bins, then \n",
    "#it simply appends 0 in that missing time_bin else it adds the original number of pickups in that time_bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MezYj258Ya1I"
   },
   "outputs": [],
   "source": [
    "def smoothing(numberOfPickups, correspondingTimeBin):\n",
    "    ind = 0\n",
    "    repeat = 0\n",
    "    smoothed_region = []\n",
    "    for cluster in range(0, 30):\n",
    "        smoothed_bin = []\n",
    "        for t1 in range(4464):\n",
    "            if repeat != 0:   #this will ensure that we shall not fill the pickup values again which we already filled by smoothing\n",
    "                repeat -= 1\n",
    "            else:\n",
    "                if t1 in correspondingTimeBin[cluster]:\n",
    "                    smoothed_bin.append(numberOfPickups[ind])\n",
    "                    ind += 1\n",
    "                else:\n",
    "                    if t1 == 0:           \n",
    "    #<---------------------CASE-1:Pickups missing in the beginning------------------------>\n",
    "                        for t2 in range(t1, 4464):\n",
    "                            if t2 not in correspondingTimeBin[cluster]:\n",
    "                                continue\n",
    "                            else:\n",
    "                                right_hand_limit = t2\n",
    "                                smoothed_value = (numberOfPickups[ind]*1.0)/((right_hand_limit + 1)*1.0)\n",
    "                                for i in range(right_hand_limit + 1):\n",
    "                                    smoothed_bin.append(math.ceil(smoothed_value))\n",
    "                                ind += 1\n",
    "                                repeat = right_hand_limit - t1\n",
    "                                \n",
    "                    if t1 != 0:\n",
    "                        right_hand_limit = 0\n",
    "                        for t2 in range(t1, 4464):\n",
    "                            if t2 not in correspondingTimeBin[cluster]:\n",
    "                                continue\n",
    "                            else:\n",
    "                                right_hand_limit = t2\n",
    "                                break\n",
    "                        if right_hand_limit == 0:\n",
    "    #<---------------------CASE-2: Pickups MISSING IN THE END------------------------------>\n",
    "                            smoothed_value = (numberOfPickups[ind-1]*1.0)/(((4464 - t1)+1)*1.0)\n",
    "                            del smoothed_bin[-1]\n",
    "                            for i in range((4464 - t1)+1):\n",
    "                                smoothed_bin.append(math.ceil(smoothed_value))\n",
    "                            repeat = (4464 - t1) - 1    \n",
    "    #<---------------------CASE-3: Pickups MISSING IN MIDDLE OF TWO VALUES----------------> \n",
    "                        else: \n",
    "                            smoothed_value = ((numberOfPickups[ind-1] + numberOfPickups[ind])*1.0)/(((right_hand_limit - t1)+2)*1.0)\n",
    "                            del smoothed_bin[-1]\n",
    "                            for i in range((right_hand_limit - t1)+2):\n",
    "                                smoothed_bin.append(math.ceil(smoothed_value))\n",
    "                            ind += 1\n",
    "                            repeat = right_hand_limit - t1                        \n",
    "        smoothed_region.extend(smoothed_bin)\n",
    "    return smoothed_region\n",
    "\n",
    "# when we multiply any integer with \"1.0\", then it will be converted into float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1E7IlihYa1L"
   },
   "outputs": [],
   "source": [
    "jan_2015_fillZero = fillMissingWithZero(jan_2015_timeBin_groupBy[\"trip_distance\"].values, unique_binswithPickup_Jan_2015)\n",
    "# here in jan_2015_timeBin_groupBy dataframe the \"trip_distance\" represents the number of pickups that are happened.\n",
    "jan_2015_fillSmooth = smoothing(jan_2015_timeBin_groupBy[\"trip_distance\"].values, unique_binswithPickup_Jan_2015)\n",
    "\n",
    "#\"unique_binswithPickup_Jan_2015\" contains all the unique time bins, where pickup happened. It contains 30 sub-arrays as there are 30 clusters\n",
    "#and each sub-array contains the unique ID of all the time bins where pickup happened in the clusters which is the index of that sub-array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kG05oBnOYa1N"
   },
   "outputs": [],
   "source": [
    "def countZeros(num):\n",
    "    count = 0\n",
    "    for i in num:\n",
    "        if i == 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2qWTEMeYa1P"
   },
   "outputs": [],
   "source": [
    "print(\"Number of values filled with zero in zero fill data= \"+str(countZeros(jan_2015_fillZero)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiidR2G8Ya1R"
   },
   "outputs": [],
   "source": [
    "print(\"Sanity check for number of zeros in smoothed data = \"+str(countZeros(jan_2015_fillSmooth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OaQ2KVcLYa1U"
   },
   "source": [
    "There are total 30 clusters. Each cluster has 4464 time bins. After smoothing or fillWithZero, each of 4464 time bin has a pickup. So, there should be a total of <b>4464*30 = 133920</b> pickup values present for the month of January 2015. Let's Check, is it correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pe1BXJDPYa1V"
   },
   "outputs": [],
   "source": [
    "print(\"Total number of pickup values = \"+str(len(jan_2015_fillZero)))\n",
    "print(\"Total number of pickup values = \"+str(len(jan_2015_fillSmooth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5sZ3nqTlYa1b"
   },
   "source": [
    "#### Above plot is a plot of all the pickup values for cluster 26th. It shows that all the zero filled are separated by avg filled values. The minimum average filled value is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5ewkOzIYa1c"
   },
   "outputs": [],
   "source": [
    "unique_binswithPickup_Jan_2016 = getUniqueBinsWithPickups(jan_2016_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1J_H9DIYa1d"
   },
   "outputs": [],
   "source": [
    "# Jan-2015 data is smoothed, Jan-2016 data missing values are filled with zero\n",
    "jan_2016_fillZero = fillMissingWithZero(jan_2016_timeBin_groupBy[\"trip_distance\"].values, unique_binswithPickup_Jan_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_L5zQ8RYa1e"
   },
   "outputs": [],
   "source": [
    "regionWisePickup_Jan_2016 = []\n",
    "for i in range(30):\n",
    "    regionWisePickup_Jan_2016.append(jan_2016_fillZero[4464*i:((4464*i)+4464)])\n",
    "#\"regionWisePickup_Jan_2016\" is a list of lists which contains 30 sub lists, where the index of each sub-list is the \n",
    "#corresponding cluster number and the element of each sub-list is the pickup value. So, we know that there are 4464 time bins \n",
    "#in Jan 2016, hence, each sub-list is of size 4464."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqPmj5NOYa1f"
   },
   "outputs": [],
   "source": [
    "print(len(regionWisePickup_Jan_2016))\n",
    "print(len(regionWisePickup_Jan_2016[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xe5kmHGYa1g"
   },
   "outputs": [],
   "source": [
    "def find_missing_timeBins(IDs):\n",
    "    missing = []\n",
    "    for i in range(len(unique_binswithPickup_Jan_2015[IDs]) - 1):\n",
    "        j = unique_binswithPickup_Jan_2015[IDs][i]\n",
    "        k = unique_binswithPickup_Jan_2015[IDs][i+1]\n",
    "        if (k-j) > 1:\n",
    "            for l in range(j+1, k):\n",
    "                missing.append(l)\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXenuqV3Ya1h"
   },
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdJdBQmuYa1h"
   },
   "source": [
    "Now we get into modelling in order to forecast the pickup densities for the months of Jan-2016 for which we are using multiple models with two variations. \n",
    "1. Using Ratios of the 2016 data to the 2015 data i.e $R_{t} = P^{2016}_{t} / P^{2015}_{t}$\n",
    "2. Using Previous known values of the 2016 data itself to predict the future values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lOEj57PqYa1h"
   },
   "source": [
    "### Preparing dataframe with $x_i$ as Jan- 2015 pickups and $y_i$ as Jan-2016 Pickups, with ratios as $Pickup^{2016} / Pickup^{2015}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ApdNJQUvYa1i"
   },
   "outputs": [],
   "source": [
    "Ratios_DF = pd.DataFrame()\n",
    "Ratios_DF[\"Given\"] = jan_2015_fillSmooth\n",
    "Ratios_DF[\"Prediction\"] = jan_2016_fillZero\n",
    "Ratios_DF[\"Ratio\"] = Ratios_DF[\"Prediction\"]*1.0/Ratios_DF[\"Given\"]*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7PGAjihYa1j"
   },
   "outputs": [],
   "source": [
    "Ratios_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNuRxkXsYa1k"
   },
   "outputs": [],
   "source": [
    "Ratios_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9SSOGMXYa1l"
   },
   "outputs": [],
   "source": [
    "print(\"Total Number of zeros in Ratio column = \"+str(Ratios_DF[\"Ratio\"].value_counts()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IsJwdP5JYa1m"
   },
   "outputs": [],
   "source": [
    "print(\"Total Number of zeros in Prediction column = \"+str(Ratios_DF[\"Prediction\"].value_counts()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrIIZMQUYa1v"
   },
   "source": [
    "## Simple Weighted Moving Average\n",
    "The Moving Avergaes Model used gave equal importance to all the values in the window used, but we know intuitively that the future is more likely to be similar to the latest values and less similar to the older values. Weighted Averages converts this analogy into a mathematical relationship giving the highest weight while computing the averages to the latest previous value and decreasing weights to the subsequent older ones<br>\n",
    "\n",
    "Weighted Moving Averages using Ratio Values: $R_t = (N*R_{t-1} + N-1*R_{t-2} + N-2*R_{T-3} + ...) / N*(N+1)/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeaCfY4TYa1x"
   },
   "source": [
    "For the above the Hyperparameter is the window-size (n) which is tuned manually and it is found that the window-size of 4 is optimal for getting the best results using Weighted Moving Averages using previous Ratio values therefore we get $ R_{t} = ( 4*R_{t-1} + 3*R_{t-2} + 2*R_{t-3} + 1*R_{t-4})/10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6Juac-4Ya1x"
   },
   "source": [
    "Weighted Moving Averages using Previous 2016 Values - $P_{t} = ( N*P_{t-1} + (N-1)*P_{t-2} + (N-2)*P_{t-3} .... 1*P_{t-n} )/(N*(N+1)/2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wr1FwlAmYa1y"
   },
   "outputs": [],
   "source": [
    "def weighted_moving_average_predictions(ratios):\n",
    "    predicted_pickup = (ratios[\"Prediction\"].values)[0]\n",
    "    predicted_pickup_values = []\n",
    "    absolute_error = []\n",
    "    squared_error = []\n",
    "    window_size = 2\n",
    "    for i in range(4464*30):\n",
    "        if i % 4464 == 0:\n",
    "            predicted_pickup_values.append(0)\n",
    "            absolute_error.append(0)\n",
    "            squared_error.append(0)\n",
    "        else:\n",
    "            predicted_pickup_values.append(predicted_pickup)\n",
    "            absolute_error.append(abs(predicted_pickup_values[i] - ratios[\"Prediction\"].values[i]))\n",
    "            \n",
    "            error = math.pow(int(predicted_pickup_values[i] - ratios[\"Prediction\"].values[i]), 2)\n",
    "            squared_error.append(error)\n",
    "            \n",
    "        if (i+1)>=window_size:\n",
    "            sumPickups = 0\n",
    "            sumOfWeights = 0\n",
    "            for j in range(window_size, 0, -1):\n",
    "                sumPickups = sumPickups + j*(ratios[\"Prediction\"].values)[i -window_size + j]\n",
    "                sumOfWeights = sumOfWeights + j\n",
    "            predicted_pickup = sumPickups/sumOfWeights\n",
    "        else:\n",
    "            sumPickups = 0\n",
    "            sumOfWeights = 0\n",
    "            for j in range(i+1, 0, -1):\n",
    "                sumPickups += j*(ratios[\"Prediction\"].values)[j-1]\n",
    "                sumOfWeights += j\n",
    "            predicted_pickup = sumPickups/sumOfWeights\n",
    "    \n",
    "    ratios[\"Weighted_Moving_Average_Predictions_Pred\"] = predicted_pickup_values\n",
    "    ratios[\"Weighted_Moving_Average_Predictions_AbsError\"] = absolute_error\n",
    "    mean_absolute_percentage_error = (sum(absolute_error)/len(absolute_error)) / (sum(ratios[\"Prediction\"]) / len(ratios[\"Prediction\"]))\n",
    "    mean_sq_error = sum(squared_error)/len(squared_error)\n",
    "    return ratios, mean_absolute_percentage_error, mean_sq_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l8Sa8qIAYa10"
   },
   "source": [
    "For the above the Hyperparameter is the window-size (n) which is tuned manually and it is found that the window-size of 2 is optimal for getting the best results using Weighted Moving Averages using previous Ratio values therefore we get $ P_t = ( 2*P_{t-1} + 1*P_{t-2})/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufDdnaK3Ya13"
   },
   "source": [
    "Here, above, alpha is a hyper-parameter which needs to be tuned manually. It is found that alpha = 0.5 gives lowest MAPE value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p4id0ODeYa15"
   },
   "outputs": [],
   "source": [
    "r4, mape4, mse4 = weighted_moving_average_predictions(Ratios_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k98fYZCuYa16"
   },
   "outputs": [],
   "source": [
    "error_table_baseline = pd.DataFrame(columns = [\"Model\", \"MAPE(%)\", \"MSE\"])\n",
    "\n",
    "error_table_baseline = error_table_baseline.append(pd.DataFrame([[\"Weighted Moving Average Predictions\", mape4*100, mse4]], columns = [\"Model\", \"MAPE(%)\", \"MSE\"]))\n",
    "\n",
    "error_table_baseline.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9-oL8jLYa17"
   },
   "outputs": [],
   "source": [
    "error_table_baseline.style.highlight_min(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-B43GSKtYa18"
   },
   "source": [
    "<b>Plese Note:-</b> The above comparisons are made using Jan 2015 and Jan 2016 only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dw9-bJ2PYa18"
   },
   "source": [
    "<b>From the above error table it is inferred that the best forecasting model for our prediction would be:-</b>\n",
    "$P_{t} = ( N*P_{t-1} + (N-1)*P_{t-2} + (N-2)*P_{t-3} .... 1*P_{t-n} )/(N*(N+1)/2)$ i.e Weighted Moving Averages Predictions using 2016 Values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlLF9fCiYa18"
   },
   "source": [
    "# Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-ryPltqYa19"
   },
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttCAs1tQYa19"
   },
   "source": [
    "Preparing data to be split into train and test, The below code prepares data in cumulative form which will be later split into\n",
    "test and train\n",
    "\n",
    "There are total 30 clusters and for the month of January-2016 and there are total 4464 time bins. \n",
    "For each cluster region there are 4464 time bins and so, for 30 clusters there will be 4464*30 pickup values because after \n",
    "smoothing each time bin has pickup.\n",
    "We will have a total of 4464*30 = 133920 pickup values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hs-6BHAjYa19"
   },
   "outputs": [],
   "source": [
    "# we take number of pickups that are happened in last 5 10min intravels\n",
    "number_of_time_stamps = 5\n",
    "\n",
    "# TruePickups varaible\n",
    "# it is list of lists\n",
    "# It will be used as true labels/ground truth. Now since we are taking previous 5 pickups as a training data for predicting\n",
    "# next pickup(here next pickup will be a true/ground truth pickup), so \"TruePickups\" will not contain first five pickups of each \n",
    "# cluster. It will contain number of pickups 4459 for each cluster. \n",
    "TruePickups = []\n",
    "\n",
    "# lat will contain 4464-5=4459 times latitude of cluster center for every cluster.\n",
    "# Ex: [[cent_lat 4459 times],[cent_lat 4459 times], [cent_lat 4459 times].... 30 lists]\n",
    "# it is list of lists\n",
    "lat = []\n",
    "\n",
    "# lon will contain 4464-5=4459 times longitude of cluster center for every cluster.\n",
    "# Ex: [[cent_lat 4459 times],[cent_lat 4459 times], [cent_lat 4459 times].... 30 lists]\n",
    "# it is list of lists\n",
    "lon = []\n",
    "\n",
    "# we will code each day \n",
    "# sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5,sat=6\n",
    "day_of_week = []\n",
    "\n",
    "\n",
    "# for every cluster we will be adding 4459 values, each value represent to which day of the week that pickup bin belongs to\n",
    "# it is list of lists\n",
    "\n",
    "# feat is a numbpy array, of shape (133770, 5). {4459*30 = 133770.}\n",
    "# each row corresponds to an entry in our data\n",
    "# for the first row we will have [f0,f1,f2,f3,f4] fi=number of pickups happened in i+1st 10min intravel(bin)\n",
    "# the second row will have [f1,f2,f3,f4,f5]\n",
    "# the third row will have [f2,f3,f4,f5,f6]\n",
    "# and so on...\n",
    "feat = []\n",
    "\n",
    "\n",
    "centerOfRegions = regions.cluster_centers_\n",
    "feat = [0]*number_of_time_stamps\n",
    "for i in range(30):\n",
    "    lat.append([centerOfRegions[i][0]]*4459) \n",
    "    lon.append([centerOfRegions[i][1]]*4459)\n",
    "    #1 January 2016 is a Friday so we start our day from 5: \"(int(j/144))%7+5\"\n",
    "    # Our prediction starts from 5th 10min interval since we need to have number of pickups that are happened in last 5 pickup bins.\n",
    "    day_of_week.append([int(((int(j/144)%7)+5)%7) for j in range(5, 4464)])\n",
    "    #\"regionWisePickup_Jan_2016\" is a list of lists which contains 30 sub lists, where the index of each sub-list is the \n",
    "    #corresponding cluster number and the element of each sub-list is the pickup value. So, we know that there are 4464 time bins \n",
    "    #in Jan 2016, hence, each sub-list is of size 4464.\n",
    "    #\"regionWisePickup_Jan_2016\" is a list of lists [[x1,x2,x3..4464], [x1,x2,x3..4464], [x1,x2,x3..4464], [x1,x2,x3..4464], [x1,x2,x3..4464], .. 30 lists]\n",
    "    #Here, x1,x2,x3... are pickup values at time stamp 1,2,3... respectively\n",
    "    feat = np.vstack((feat, [regionWisePickup_Jan_2016[i][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_Jan_2016[i]) - (number_of_time_stamps))]))\n",
    "    TruePickups.append(regionWisePickup_Jan_2016[i][5:])\n",
    "    #output contains pickup values of all the regions and of each time stamp, except first 5 time stamp pickups of each region.\n",
    "feat = feat[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDUNlVRgYa1-"
   },
   "outputs": [],
   "source": [
    "len(lat[0])*len(lat) == len(lon[0])*len(lon) == len(day_of_week[0])*len(day_of_week) == 4459*30 == len(feat) == len(TruePickups[0])*len(TruePickups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wOxZcXhsYa1_"
   },
   "outputs": [],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgcjmewbYa2A"
   },
   "source": [
    "### Adding Predictions of Weighted Moving Average Predictions as a feature in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQvaUxe1Ya2A"
   },
   "source": [
    "Getting the predictions of weighted moving averages to be used as a feature in cumulative form.\n",
    "\n",
    "Upto now we computed 8 features for every data point that starts from 50th min of the day.\n",
    "1. cluster center latitude\n",
    "2. cluster center longitude\n",
    "3. day of the week \n",
    "4. f_t_1: number of pickups that are happened previous t-1st 10min interval\n",
    "5. f_t_2: number of pickups that are happened previous t-2nd 10min interval\n",
    "6. f_t_3: number of pickups that are happened previous t-3rd 10min interval\n",
    "7. f_t_4: number of pickups that are happened previous t-4th 10min interval\n",
    "8. f_t_5: number of pickups that are happened previous t-5th 10min interval\n",
    "\n",
    "From the baseline models we said that the weighted moving avarage predictions gives us the best error.\n",
    "We will try to add the same weighted moving avarage predictions at time t as a feature to our data.<br>\n",
    "Weighted Moving Average -> $P_{t} = ( N*P_{t-1} + (N-1)*P_{t-2} + (N-2)*P_{t-3} .... 1*P_{t-n} )/(N*(N+1)/2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCjYS9urYa2B"
   },
   "outputs": [],
   "source": [
    "new_frame_cleaned.head()# \"predicted_pickup_values\": it is a temporary array that store weighted moving avarag prediction values for each 10min intervl, \n",
    "# for each cluster it will get reset.\n",
    "# for every cluster it contains 4464 values\n",
    "predicted_pickup_values = []\n",
    "\n",
    "# \"predicted_pickup_values_list\"\n",
    "# it is list of lists\n",
    "# predict_list is a list of lists [[x5,x6,x7..x4463], [x5,x6,x7..x4463], [x5,x6,x7..x4463], ... 40 lists]\n",
    "predicted_pickup_values_list = []\n",
    "\n",
    "predicted_value = -1  #it will contain cuurent predicted_value. Default is given -1 which will be replaced later\n",
    "\n",
    "window_size = 2\n",
    "for i in range(30):\n",
    "    for j in range(4464):\n",
    "        if j == 0:\n",
    "            predicted_value = regionWisePickup_Jan_2016[i][j]\n",
    "            predicted_pickup_values.append(0)\n",
    "        else:\n",
    "            if j>=window_size:\n",
    "                sumPickups = 0\n",
    "                sumOfWeights = 0\n",
    "                for k in range(window_size, 0, -1):\n",
    "                    sumPickups += k*(regionWisePickup_Jan_2016[i][j -window_size + (k - 1)])\n",
    "                    sumOfWeights += k\n",
    "                predicted_value = int(sumPickups/sumOfWeights)\n",
    "                predicted_pickup_values.append(predicted_value)\n",
    "            else:\n",
    "                sumPickups = 0\n",
    "                sumOfWeights = 0\n",
    "                for k in range(j, 0, -1):\n",
    "                    sumPickups += k*regionWisePickup_Jan_2016[i][k-1]\n",
    "                    sumOfWeights += k\n",
    "                predicted_value = int(sumPickups/sumOfWeights)\n",
    "                predicted_pickup_values.append(predicted_value)\n",
    "                \n",
    "    predicted_pickup_values_list.append(predicted_pickup_values[5:])\n",
    "    predicted_pickup_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJCF4mPPYa2B"
   },
   "outputs": [],
   "source": [
    "len(predicted_pickup_values_list[0])*len(predicted_pickup_values_list) == 4459*30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZZ9LHRrYa2D"
   },
   "source": [
    "### Adding Top 5 Frequencies and Amplitudes of Fourier Transform as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-7ZL1NFYa2K"
   },
   "outputs": [],
   "source": [
    "amplitude_lists = []\n",
    "frequency_lists = []\n",
    "for i in range(30):\n",
    "    ampli  = np.abs(np.fft.fft(regionWisePickup_Jan_2016[i][0:4096]))\n",
    "    freq = np.abs(np.fft.fftfreq(4096, 1))\n",
    "    ampli_indices = np.argsort(-ampli)[1:]        #it will return an array of indices for which corresponding amplitude values are sorted in reverse order.\n",
    "    amplitude_values = []\n",
    "    frequency_values = []\n",
    "    for j in range(0, 9, 2):   #taking top five amplitudes and frequencies\n",
    "        amplitude_values.append(ampli[ampli_indices[j]])\n",
    "        frequency_values.append(freq[ampli_indices[j]])\n",
    "    for k in range(4459):    #those top 5 frequencies and amplitudes are same for all the points in one cluster\n",
    "        amplitude_lists.append(amplitude_values)\n",
    "        frequency_lists.append(frequency_values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ri-KPIeIYa2L"
   },
   "source": [
    "<b>Now we have built our all the features. We have finally now following 19 features in our data:</b>\n",
    "1. <b>f_t_1:</b> Number of pickups that are happened previous t-1st 10min interval\n",
    "2. <b>f_t_2:</b> Number of pickups that are happened previous t-2nd 10min interval\n",
    "3. <b>f_t_3:</b> Number of pickups that are happened previous t-3rd 10min interval\n",
    "4. <b>f_t_4:</b> Number of pickups that are happened previous t-4th 10min interval\n",
    "5. <b>f_t_5:</b> Number of pickups that are happened previous t-5th 10min interval \n",
    "6. <b>Freq1:</b> Fourier Frequency corresponding to 1st highest amplitude\n",
    "7. <b>Freq2:</b> Fourier Frequency corresponding to 2nd highest amplitude\n",
    "8. <b>Freq3:</b> Fourier Frequency corresponding to 3rd highest amplitude\n",
    "9. <b>Freq4:</b> Fourier Frequency corresponding to 4th highest amplitude\n",
    "10. <b>Freq5:</b> Fourier Frequency corresponding to 5th highest amplitude\n",
    "11. <b>Amp1:</b>  Amplitude corresponding to 1st highest fourier transformed wave.\n",
    "12. <b>Amp2:</b>  Amplitude corresponding to 2nd highest fourier transformed wave.\n",
    "13. <b>Amp3:</b>  Amplitude corresponding to 3rd highest fourier transformed wave.\n",
    "14. <b>Amp4:</b>  Amplitude corresponding to 4th highest fourier transformed wave.\n",
    "15. <b>Amp5:</b>  Amplitude corresponding to 5th highest fourier transformed wave.\n",
    "16. <b>Latitude:</b> Latitude of Cluster center.\n",
    "17. <b>Longitude:</b> Longitude of Cluster Center.\n",
    "18. <b>WeekDay:</b> Day of week of pickup.\n",
    "19. <b>WeightedAvg:</b>: Weighted Moving Average Prediction values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "unJ_-uRTYa2L"
   },
   "source": [
    "## Data Preparation for regression models\n",
    "Before we start predictions using the tree based regression models we take Jan 2016 pickup data and split it such that for every region we have 80% data in train and 20% in test, ordered date-wise for every region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbonS5gDYa2M"
   },
   "outputs": [],
   "source": [
    "print(\"size of total train data :\" +str(int(133770*0.8)))\n",
    "print(\"size of total test data :\" +str(int(133770*0.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szdA1SdyYa2P"
   },
   "outputs": [],
   "source": [
    "print(\"size of train data for one cluster:\" +str(int(4459*0.8)))\n",
    "print(\"size of total test data for one cluster:\" +str(int(4459*0.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCPGW8DeYa2S"
   },
   "outputs": [],
   "source": [
    "train_previousFive_pickups  = [feat[i*4459:(4459*i+3567)] for i in range(30)]\n",
    "test_previousFive_pickups  = [feat[(i*4459)+3567:(4459*(i+1))] for i in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UA4PEV29Ya2T"
   },
   "outputs": [],
   "source": [
    "train_fourier_frequencies = [frequency_lists[i*4459:(4459*i+3567)] for i in range(30)]\n",
    "test_fourier_frequencies = [frequency_lists[(i*4459)+3567:(4459*(i+1))] for i in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQhn-NMUYa2U"
   },
   "outputs": [],
   "source": [
    "train_fourier_amplitudes = [amplitude_lists[i*4459:(4459*i+3567)] for i in range(30)]\n",
    "test_fourier_amplitudes = [amplitude_lists[(i*4459)+3567:(4459*(i+1))] for i in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oY-lhfXrYa2W"
   },
   "outputs": [],
   "source": [
    "print(\"Train Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of training points = {}\".format(len(train_previousFive_pickups), len(train_previousFive_pickups[0]), len(train_previousFive_pickups)*len(train_previousFive_pickups[0])))\n",
    "print(\"Test Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of test points = {}\".format(len(test_previousFive_pickups), len(test_previousFive_pickups[0]), len(test_previousFive_pickups)*len(test_previousFive_pickups[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YMcnIAwYa2Z"
   },
   "outputs": [],
   "source": [
    "#taking 80% data as train data from each cluster\n",
    "train_lat = [i[:3567] for i in lat]\n",
    "train_lon = [i[:3567] for i in lon]\n",
    "train_weekDay = [i[:3567] for i in day_of_week]\n",
    "train_weighted_avg = [i[:3567] for i in predicted_pickup_values_list]\n",
    "train_TruePickups = [i[:3567] for i in TruePickups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brTXf7e2Ya2a"
   },
   "outputs": [],
   "source": [
    "#taking 20% data as test data from each cluster\n",
    "test_lat = [i[3567:] for i in lat]\n",
    "test_lon = [i[3567:] for i in lon]\n",
    "test_weekDay = [i[3567:] for i in day_of_week]\n",
    "test_weighted_avg = [i[3567:] for i in predicted_pickup_values_list]\n",
    "test_TruePickups = [i[3567:] for i in TruePickups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zF95iJyYa2b"
   },
   "outputs": [],
   "source": [
    "# convert from lists of lists of list to lists of list\n",
    "train_pickups = []\n",
    "test_pickups = []\n",
    "train_freq = []\n",
    "test_freq = []\n",
    "train_amp = []\n",
    "test_amp = []\n",
    "for i in range(30):\n",
    "    train_pickups.extend(train_previousFive_pickups[i])\n",
    "    test_pickups.extend(test_previousFive_pickups[i])\n",
    "    train_freq.extend(train_fourier_frequencies[i])\n",
    "    test_freq.extend(test_fourier_frequencies[i])\n",
    "    train_amp.extend(train_fourier_amplitudes[i])\n",
    "    test_amp.extend(test_fourier_amplitudes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ew2vV-fYa2d"
   },
   "outputs": [],
   "source": [
    "#stacking pickups,frequencies and amplitudes horizontally.\n",
    "\n",
    "train_prevPickups_freq_amp = np.hstack((train_pickups, train_freq, train_amp))\n",
    "test_prevPickups_freq_amp = np.hstack((test_pickups, test_freq, test_amp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AaZyvS7ZYa2e"
   },
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data = {}. Number of columns till now = {}\".format(len(train_prevPickups_freq_amp), len(train_prevPickups_freq_amp[0])))\n",
    "print(\"Number of data points in test data = {}. Number of columns till now = {}\".format(len(test_prevPickups_freq_amp), len(test_prevPickups_freq_amp[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8UJZrSE4Ya2f"
   },
   "outputs": [],
   "source": [
    "# converting lists of lists into single list i.e flatten\n",
    "\n",
    "train_flat_lat = sum(train_lat, [])\n",
    "train_flat_lon = sum(train_lon, [])\n",
    "train_flat_weekDay = sum(train_weekDay, [])\n",
    "train_weighted_avg_flat = sum(train_weighted_avg, [])\n",
    "train_TruePickups_flat = sum(train_TruePickups, [])\n",
    "\n",
    "test_flat_lat = sum(test_lat, [])\n",
    "test_flat_lon = sum(test_lon, [])\n",
    "test_flat_weekDay = sum(test_weekDay, [])\n",
    "test_weighted_avg_flat = sum(test_weighted_avg, [])\n",
    "test_TruePickups_flat = sum(test_TruePickups, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJaAOn64Ya2f"
   },
   "outputs": [],
   "source": [
    "#train dataframe\n",
    "columns = ['ft_5','ft_4','ft_3','ft_2','ft_1', 'freq1', 'freq2','freq3','freq4','freq5', 'Amp1', 'Amp2', 'Amp3', 'Amp4', 'Amp5']\n",
    "Train_DF = pd.DataFrame(data = train_prevPickups_freq_amp, columns = columns)\n",
    "Train_DF[\"Latitude\"] = train_flat_lat\n",
    "Train_DF[\"Longitude\"] = train_flat_lon\n",
    "Train_DF[\"WeekDay\"] = train_flat_weekDay\n",
    "Train_DF[\"WeightedAvg\"] = train_weighted_avg_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3qS8zoU8Ya2h"
   },
   "outputs": [],
   "source": [
    "#test dataframe\n",
    "Test_DF = pd.DataFrame(data = test_prevPickups_freq_amp, columns = columns)\n",
    "Test_DF[\"Latitude\"] = test_flat_lat\n",
    "Test_DF[\"Longitude\"] = test_flat_lon\n",
    "Test_DF[\"WeekDay\"] = test_flat_weekDay\n",
    "Test_DF[\"WeightedAvg\"] = test_weighted_avg_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zl7ZTShCYa2i"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of train data = \"+str(Train_DF.shape))\n",
    "print(\"Shape of test data = \"+str(Test_DF.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cbOK_a4Ya2l"
   },
   "outputs": [],
   "source": [
    "Train_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3ZQxk-IYa2o"
   },
   "outputs": [],
   "source": [
    "Test_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-RSlYPcpYa2p"
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-QczMg7Ya2r"
   },
   "outputs": [],
   "source": [
    "def lin_regression(train_data, train_true, test_data, test_true):\n",
    "    \n",
    "    #standardizing the data\n",
    "    train_std = StandardScaler().fit_transform(train_data)\n",
    "    test_std = StandardScaler().fit_transform(test_data)\n",
    "    \n",
    "    #hyper-paramater tuning\n",
    "    clf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\")\n",
    "    values = [10**-14, 10**-12, 10**-10, 10**-8, 10**-6, 10**-4, 10**-2, 10**0, 10**2, 10**4, 10**6]\n",
    "    hyper_parameter = {\"alpha\": values}\n",
    "    best_parameter = GridSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\n",
    "    best_parameter.fit(train_std, train_true)\n",
    "    alpha = best_parameter.best_params_[\"alpha\"]\n",
    "    \n",
    "    #applying linear regression with best hyper-parameter\n",
    "    clf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\", alpha = alpha)\n",
    "    clf.fit(train_std, train_true)\n",
    "    train_pred = clf.predict(train_std)\n",
    "    train_MAPE = mean_absolute_error(train_true, train_pred)/ (sum(train_true)/len(train_true))\n",
    "    train_MSE = mean_squared_error(train_true, train_pred)\n",
    "    test_pred = clf.predict(test_std)\n",
    "    test_MAPE = mean_absolute_error(test_true, test_pred)/ (sum(test_true)/len(test_true))\n",
    "    test_MSE = mean_squared_error(test_true, test_pred)\n",
    "    \n",
    "    return train_MAPE, train_MSE, test_MAPE, test_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6ifEsh9Ya2t"
   },
   "source": [
    "## XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRelFf_PYa2t"
   },
   "outputs": [],
   "source": [
    "def xgboost_reg(train_data, train_true, test_data, test_true):\n",
    "    #hyper-parameter tuning\n",
    "    hyper_parameter = {\"max_depth\":[1, 2, 3, 4], \"n_estimators\":[40, 80, 150, 600]}\n",
    "    clf = xgb.XGBRegressor()\n",
    "    best_parameter = GridSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\n",
    "    best_parameter.fit(train_data, train_true)\n",
    "    estimators = best_parameter.best_params_[\"n_estimators\"]\n",
    "    depth = best_parameter.best_params_[\"max_depth\"]\n",
    "    \n",
    "    #applying xgboost regressor with best hyper-parameter\n",
    "    clf = xgb.XGBRegressor(max_depth = depth, n_estimators = estimators)\n",
    "    clf.fit(train_data, train_true)\n",
    "    train_pred = clf.predict(train_data)\n",
    "    train_MAPE = mean_absolute_error(train_true, train_pred)/ (sum(train_true)/len(train_true))\n",
    "    train_MSE = mean_squared_error(train_true, train_pred)\n",
    "    test_pred = clf.predict(test_data)\n",
    "    test_MAPE = mean_absolute_error(test_true, test_pred)/ (sum(test_true)/len(test_true))\n",
    "    test_MSE = mean_squared_error(test_true, test_pred)\n",
    "    \n",
    "    return train_MAPE, train_MSE, test_MAPE, test_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5tHnz2DWYa2v"
   },
   "outputs": [],
   "source": [
    "trainMAPE_lr, trainMSE_lr, testMAPE_lr, testMSE_lr = lin_regression(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "\n",
    "trainMAPE_xgb, trainMSE_xgb, testMAPE_xgb, testMSE_xgb = xgboost_reg(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04QT8jy2Ya2w"
   },
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMA9fBzQYa2w"
   },
   "outputs": [],
   "source": [
    "error_table_regressions = pd.DataFrame(columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TestMAPE(%)\", \"TestMSE\"])\n",
    "\n",
    "error_table_regressions = error_table_regressions.append(pd.DataFrame([[\"Linear Regression\", trainMAPE_lr*100, trainMSE_lr, testMAPE_lr*100, testMSE_lr ]], columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TestMAPE(%)\", \"TestMSE\"]))\n",
    "\n",
    "error_table_regressions = error_table_regressions.append(pd.DataFrame([[\"XGBoost Regressor\", trainMAPE_xgb*100, trainMSE_xgb, testMAPE_xgb*100, testMSE_xgb]], columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TestMAPE(%)\", \"TestMSE\"]))\n",
    "error_table_regressions.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDfHnoGIYa2x"
   },
   "outputs": [],
   "source": [
    "error_table_regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vZovQ6xYa20"
   },
   "outputs": [],
   "source": [
    "Final_Table = pd.DataFrame(columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"])\n",
    "\n",
    "Final_Table = Final_Table.append(pd.DataFrame([[\"Weighted Moving Average Predictions\", mape4*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "\n",
    "Final_Table = Final_Table.append(pd.DataFrame([[\"Linear Regression\", testMAPE_lr*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "\n",
    "Final_Table = Final_Table.append(pd.DataFrame([[\"XGBoost Regressor\", testMAPE_xgb*100]], columns = [\"Model\", \"Mean_Absolute_Per_Error(%)\"]))\n",
    "\n",
    "Final_Table.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rp_iL9D5Ya21"
   },
   "outputs": [],
   "source": [
    "ax = Final_Table.plot(x = \"Model\", kind = \"bar\", figsize = (12, 8), grid = True, fontsize = 15)\n",
    "ax.set_title(\"Test MAPE of all Models\", fontsize = 25)\n",
    "ax.set_ylabel(\"Mean Absolute Percentage Error(%)\", fontsize = 15)\n",
    "\n",
    "for i in ax.patches:   #ax.patches is an array which gives x position, y position, width of a bar graphs.\n",
    "    ax.text(i.get_x()-.05, i.get_height()+0.19, str(round(i.get_height(), 2))+'%', fontsize=14, color='black')\n",
    "#     ax.text(x, y, annotate_text, font_size, color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_DDPrOnYa22"
   },
   "outputs": [],
   "source": [
    "Final_Table.style.highlight_min(axis=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eOiXk_l_Yayb",
    "I99QDXT1Yayf",
    "L8lBioAWYayg",
    "Wq3fwjGGYayk",
    "u2dzwaFQYayo",
    "BSp4PufHYayx",
    "mxblmT1sYayy",
    "49fbtQPIYazC",
    "LBBC4QuRYazJ",
    "lC2snNshYazb",
    "DB-_UcXoYazi",
    "2xI9pXCpYazj",
    "Nj8OPjqMYaz1",
    "our9tR44Ya0I",
    "3pWO_BG0Ya0J",
    "xMzuG2gtYa0P",
    "vIK7yxfiYa0U",
    "z_8n46AbYa0b",
    "UJltlrztYa0e",
    "Vt6yhFcGYa0j",
    "nJPFXlnAYa0m",
    "EsOBbcPHYa0p",
    "ntvadQLOYa02",
    "0_IStlLrYa0_",
    "5sZ3nqTlYa1b",
    "lOEj57PqYa1h",
    "LnW1I9uKYa1n",
    "JrIIZMQUYa1v",
    "m10MaQdWYa11",
    "P-ryPltqYa19",
    "YgcjmewbYa2A",
    "1ZZ9LHRrYa2D",
    "unJ_-uRTYa2L",
    "-RSlYPcpYa2p",
    "SIrVjjpSYa2s",
    "z6ifEsh9Ya2t",
    "05J9XCSlYa2z"
   ],
   "name": "Taxi-Demand-Prediction-NYC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
